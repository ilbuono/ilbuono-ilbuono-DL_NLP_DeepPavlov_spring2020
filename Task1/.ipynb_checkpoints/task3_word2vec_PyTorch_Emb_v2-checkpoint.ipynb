{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1.3: Naive word2vec (40 points)\n",
    "\n",
    "This task can be formulated very simply. Follow this [paper](https://arxiv.org/pdf/1411.2738.pdf) and implement word2vec like a two-layer neural network with matrices $W$ and $W'$. One matrix projects words to low-dimensional 'hidden' space and the other - back to high-dimensional vocabulary space.\n",
    "\n",
    "![word2vec](https://i.stack.imgur.com/6eVXZ.jpg)\n",
    "\n",
    "You can use TensorFlow/PyTorch and code from your previous task.\n",
    "\n",
    "## Results of this task: (30 points)\n",
    " * trained word vectors (mention somewhere, how long it took to train)\n",
    " * plotted loss (so we can see that it has converged)\n",
    " * function to map token to corresponding word vector\n",
    " * beautiful visualizations (PCE, T-SNE), you can use TensorBoard and play with your vectors in 3D (don't forget to add screenshots to the task)\n",
    "\n",
    "## Extra questions: (10 points)\n",
    " * Intrinsic evaluation: you can find datasets [here](http://download.tensorflow.org/data/questions-words.txt)\n",
    " * Extrinsic evaluation: you can use [these](https://medium.com/@dataturks/rare-text-classification-open-datasets-9d340c8c508e)\n",
    "\n",
    "Also, you can find any other datasets for quantitative evaluation.\n",
    "\n",
    "Again. It is **highly recommended** to read this [paper](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "\n",
    "Example of visualization in tensorboard:\n",
    "https://projector.tensorflow.org\n",
    "\n",
    "Example of 2D visualisation:\n",
    "\n",
    "![2dword2vec](https://www.tensorflow.org/images/tsne.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ilbuono/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "len(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batcher:\n",
    "    def __init__(self, window_size, corpus_path, min_freq, max_freq, max_voc_size, batch_size):\n",
    "        self.corpus_path = corpus_path\n",
    "        self.window_size = window_size\n",
    "        self.min_freq = min_freq\n",
    "        self.max_freq = max_freq\n",
    "        self.max_voc_size = max_voc_size\n",
    "        self.batch_size = batch_size\n",
    "        self.words = None\n",
    "        self.word2index = None\n",
    "        self.index2word = None\n",
    "        self.freq = None\n",
    "        self.voc = None\n",
    "        self.voc_size = None\n",
    "        self.corpus = None\n",
    "        self.corpus_size = None\n",
    "        \n",
    "        \n",
    "    def read_data(self, S):\n",
    "        if S == None:\n",
    "            with open(self.corpus_path, 'r') as f:\n",
    "                S = f.read()\n",
    "            S = S.lower()[:10000000]\n",
    "        print('Len of S = ', len(S))\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        S = regex.sub(' ', S)\n",
    "        words_raw = list(S.split())\n",
    "        print(len(words_raw))\n",
    "        words = []\n",
    "        for word in words_raw:\n",
    "            if word in STOP_WORDS:\n",
    "                pass\n",
    "            else:\n",
    "                words.append(word)\n",
    "        print(len(words))\n",
    "        self.words = words\n",
    "        unique_words = list(set(words))\n",
    "        self.word2index = {k: v for v, k in enumerate(unique_words)}\n",
    "        self.word2index['UNK'] = len(unique_words)\n",
    "        self.word2index['PAD'] = len(unique_words)+1\n",
    "        self.index2word = {v: k for v, k in enumerate(unique_words)}\n",
    "        self.index2word[len(unique_words)] = 'UNK'\n",
    "        self.index2word[len(unique_words)+1] = 'PAD'\n",
    "        words = [self.word2index[word] for word in words]\n",
    "        \n",
    "        print('Size of words = ', len(words))\n",
    "        counter = Counter(words)\n",
    "        print('Size of counter = ', len(counter))\n",
    "        if self.min_freq != None:\n",
    "            counter = {x : counter[x] for x in counter if counter[x] >= self.min_freq}\n",
    "        print('Size of counter after min_freq = ', len(counter))\n",
    "        if self.max_freq != None:\n",
    "            counter = {x : counter[x] for x in counter if counter[x] <= self.max_freq}\n",
    "        print('Size of counter after max_freq = ', len(counter))\n",
    "        counter = Counter(counter)\n",
    "\n",
    "        self.freq = dict(counter.most_common(self.max_voc_size))\n",
    "        self.voc = set(self.freq)\n",
    "        self.voc_size = len(self.voc)+2\n",
    "        \n",
    "        unk = set(words).difference(self.voc)\n",
    "        print('Size of freq dict = ', len(self.voc))\n",
    "        print('Number of vocabulary words = ', len(self.voc))\n",
    "        print('Number of unknown words = ', len(unk))\n",
    "\n",
    "        words = [self.word2index['UNK'] if word in unk else word for word in words]\n",
    "        \n",
    "        if len(words)%self.batch_size == 0:\n",
    "            padding = self.window_size\n",
    "        else:\n",
    "            padding = self.batch_size - len(words)%self.batch_size + self.window_size\n",
    "            \n",
    "        self.corpus = [self.word2index['PAD']]*self.window_size + words + [self.word2index['PAD']]*padding\n",
    "        self.corpus_size = len(self.corpus)\n",
    "    \n",
    "    def generator(self):\n",
    "        i = self.window_size\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        while i < self.corpus_size-self.window_size:\n",
    "            if len(x_batch)==self.batch_size:\n",
    "                x_batch = []\n",
    "                y_batch = []\n",
    "                \n",
    "            x = self.corpus[i-self.window_size: i] + self.corpus[i+1: i+self.window_size+1]\n",
    "            y = [0]*self.voc_size\n",
    "            y[self.corpus[i]] = 1\n",
    "            #y = [self.corpus[i]]\n",
    "            x_batch.append(x)\n",
    "            y_batch.append(y)\n",
    "            i += 1\n",
    "            if len(x_batch)==self.batch_size:\n",
    "                yield np.array(x_batch), np.array(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of S =  10000000\n",
      "1706282\n",
      "1090922\n",
      "Size of words =  1090922\n",
      "Size of counter =  70835\n",
      "Size of counter after min_freq =  70835\n",
      "Size of counter after max_freq =  70835\n",
      "Size of freq dict =  70835\n",
      "Number of vocabulary words =  70835\n",
      "Number of unknown words =  0\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "batcher = Batcher(window_size=2, corpus_path='text8', min_freq=None, max_freq=None, max_voc_size=10000000, batch_size=BATCH_SIZE)\n",
    "batcher.read_data(S=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 4) (128, 70837)\n"
     ]
    }
   ],
   "source": [
    "for x, y in batcher.generator():\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f509e150ad0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# nn.CrossEntropyLos\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print (torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, voc_size, embedding_dim, window_size, batch_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embedding1 = nn.Embedding(voc_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, voc_size)\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.linear1.weight)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embs1 = self.embedding1(torch.tensor(inputs))\n",
    "        z1 = self.linear1(embs1)\n",
    "        log_softmax = F.log_softmax(z1, dim=2)\n",
    "        return log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Epoch 0 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilbuono/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/8522\n",
      "tensor(12.1713, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 10/8522\n",
      "tensor(12.0951, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 20/8522\n",
      "tensor(12.0285, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 30/8522\n",
      "tensor(12.0098, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 40/8522\n",
      "tensor(11.9959, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 50/8522\n",
      "tensor(11.9529, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 60/8522\n",
      "tensor(11.9134, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 70/8522\n",
      "tensor(11.9069, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 80/8522\n",
      "tensor(11.7805, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 90/8522\n",
      "tensor(11.8536, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 100/8522\n",
      "tensor(11.8644, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 110/8522\n",
      "tensor(11.7691, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 120/8522\n",
      "tensor(11.7168, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 130/8522\n",
      "tensor(11.7504, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 140/8522\n",
      "tensor(11.7734, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 150/8522\n",
      "tensor(11.6877, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 160/8522\n",
      "tensor(11.4882, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 170/8522\n",
      "tensor(11.7265, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 180/8522\n",
      "tensor(11.7318, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 190/8522\n",
      "tensor(11.6667, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 200/8522\n",
      "tensor(11.7032, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 210/8522\n",
      "tensor(11.6668, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 220/8522\n",
      "tensor(11.6818, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 230/8522\n",
      "tensor(11.6290, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 240/8522\n",
      "tensor(11.6522, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 250/8522\n",
      "tensor(11.6092, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 260/8522\n",
      "tensor(11.5630, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 270/8522\n",
      "tensor(11.6106, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 280/8522\n",
      "tensor(11.6066, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 290/8522\n",
      "tensor(11.6069, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 300/8522\n",
      "tensor(11.5565, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 310/8522\n",
      "tensor(11.4703, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 320/8522\n",
      "tensor(11.5269, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 330/8522\n",
      "tensor(11.5622, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 340/8522\n",
      "tensor(11.5270, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 350/8522\n",
      "tensor(11.5205, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 360/8522\n",
      "tensor(11.5182, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 370/8522\n",
      "tensor(11.4765, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 380/8522\n",
      "tensor(11.5118, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 390/8522\n",
      "tensor(11.4804, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 400/8522\n",
      "tensor(11.4660, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 410/8522\n",
      "tensor(11.5003, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 420/8522\n",
      "tensor(11.4788, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 430/8522\n",
      "tensor(11.4630, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 440/8522\n",
      "tensor(11.5218, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 450/8522\n",
      "tensor(11.4054, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 460/8522\n",
      "tensor(11.4230, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 470/8522\n",
      "tensor(11.3453, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 480/8522\n",
      "tensor(11.4266, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 490/8522\n",
      "tensor(11.4272, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 500/8522\n",
      "tensor(11.4216, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 510/8522\n",
      "tensor(11.3663, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 520/8522\n",
      "tensor(11.4269, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 530/8522\n",
      "tensor(11.4214, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 540/8522\n",
      "tensor(11.4360, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 550/8522\n",
      "tensor(11.3949, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 560/8522\n",
      "tensor(11.3434, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 570/8522\n",
      "tensor(11.4023, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 580/8522\n",
      "tensor(11.2950, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 590/8522\n",
      "tensor(11.4082, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 600/8522\n",
      "tensor(11.3823, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 610/8522\n",
      "tensor(11.3365, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 620/8522\n",
      "tensor(11.3368, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 630/8522\n",
      "tensor(11.3280, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 640/8522\n",
      "tensor(11.3480, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 650/8522\n",
      "tensor(11.3681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 660/8522\n",
      "tensor(11.3479, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 670/8522\n",
      "tensor(11.3255, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 680/8522\n",
      "tensor(11.2834, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 690/8522\n",
      "tensor(11.2989, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 700/8522\n",
      "tensor(11.3449, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 710/8522\n",
      "tensor(11.2904, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 720/8522\n",
      "tensor(11.3342, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 730/8522\n",
      "tensor(11.3514, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 740/8522\n",
      "tensor(11.3050, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 750/8522\n",
      "tensor(11.3232, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 760/8522\n",
      "tensor(11.3391, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 770/8522\n",
      "tensor(11.3303, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 780/8522\n",
      "tensor(11.3345, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 790/8522\n",
      "tensor(11.3077, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 800/8522\n",
      "tensor(11.3086, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 810/8522\n",
      "tensor(11.2859, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 820/8522\n",
      "tensor(11.2841, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 830/8522\n",
      "tensor(11.2957, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 840/8522\n",
      "tensor(11.3037, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 850/8522\n",
      "tensor(11.2971, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 860/8522\n",
      "tensor(11.2831, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 870/8522\n",
      "tensor(11.3009, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 880/8522\n",
      "tensor(11.2894, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 890/8522\n",
      "tensor(11.2582, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 900/8522\n",
      "tensor(11.3040, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 910/8522\n",
      "tensor(11.2907, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 920/8522\n",
      "tensor(11.2727, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 930/8522\n",
      "tensor(11.2812, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 940/8522\n",
      "tensor(11.2828, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 950/8522\n",
      "tensor(11.2689, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 960/8522\n",
      "tensor(11.2649, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 970/8522\n",
      "tensor(11.2615, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 980/8522\n",
      "tensor(11.2526, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 990/8522\n",
      "tensor(11.2545, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1000/8522\n",
      "tensor(11.2155, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1010/8522\n",
      "tensor(11.2470, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1020/8522\n",
      "tensor(11.2738, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1030/8522\n",
      "tensor(11.2674, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1040/8522\n",
      "tensor(11.2449, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1050/8522\n",
      "tensor(11.2390, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1060/8522\n",
      "tensor(11.2479, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1070/8522\n",
      "tensor(11.2278, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1080/8522\n",
      "tensor(11.2013, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1090/8522\n",
      "tensor(11.2546, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1100/8522\n",
      "tensor(11.2461, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1110/8522\n",
      "tensor(11.2422, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1120/8522\n",
      "tensor(11.2418, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1130/8522\n",
      "tensor(11.2525, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1140/8522\n",
      "tensor(11.2217, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1150/8522\n",
      "tensor(11.2510, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1160/8522\n",
      "tensor(11.2664, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1170/8522\n",
      "tensor(11.2371, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1180/8522\n",
      "tensor(11.2327, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1190/8522\n",
      "tensor(11.2373, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1200/8522\n",
      "tensor(11.2205, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1210/8522\n",
      "tensor(11.2336, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1220/8522\n",
      "tensor(11.2158, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1230/8522\n",
      "tensor(11.2214, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1240/8522\n",
      "tensor(11.2237, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1250/8522\n",
      "tensor(11.2292, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1260/8522\n",
      "tensor(11.2288, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1270/8522\n",
      "tensor(11.2224, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1280/8522\n",
      "tensor(11.2302, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1290/8522\n",
      "tensor(11.2162, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1300/8522\n",
      "tensor(11.2073, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1310/8522\n",
      "tensor(11.2137, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1320/8522\n",
      "tensor(11.2189, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1330/8522\n",
      "tensor(11.2177, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1340/8522\n",
      "tensor(11.2229, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1350/8522\n",
      "tensor(11.2270, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1360/8522\n",
      "tensor(11.2194, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1370/8522\n",
      "tensor(11.2178, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1380/8522\n",
      "tensor(11.2141, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1390/8522\n",
      "tensor(11.2198, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1400/8522\n",
      "tensor(11.2197, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1410/8522\n",
      "tensor(11.2136, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1420/8522\n",
      "tensor(11.1892, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1430/8522\n",
      "tensor(11.2028, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1440/8522\n",
      "tensor(11.2087, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1450/8522\n",
      "tensor(11.2109, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1460/8522\n",
      "tensor(11.1973, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1470/8522\n",
      "tensor(11.2050, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1480/8522\n",
      "tensor(11.1964, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1490/8522\n",
      "tensor(11.1916, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1500/8522\n",
      "tensor(11.2060, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1510/8522\n",
      "tensor(11.2014, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1520/8522\n",
      "tensor(11.2034, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1530/8522\n",
      "tensor(11.2056, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1540/8522\n",
      "tensor(11.2067, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1550/8522\n",
      "tensor(11.2036, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1560/8522\n",
      "tensor(11.2056, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1570/8522\n",
      "tensor(11.2012, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1580/8522\n",
      "tensor(11.2000, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1590/8522\n",
      "tensor(11.1964, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1600/8522\n",
      "tensor(11.1975, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1610/8522\n",
      "tensor(11.1979, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1620/8522\n",
      "tensor(11.1918, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1630/8522\n",
      "tensor(11.2000, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1640/8522\n",
      "tensor(11.1841, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1650/8522\n",
      "tensor(11.1988, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1660/8522\n",
      "tensor(11.1920, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1670/8522\n",
      "tensor(11.1991, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1680/8522\n",
      "tensor(11.1974, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1690/8522\n",
      "tensor(11.1992, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1700/8522\n",
      "tensor(11.1951, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1710/8522\n",
      "tensor(11.1918, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1720/8522\n",
      "tensor(11.1945, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1730/8522\n",
      "tensor(11.1941, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1740/8522\n",
      "tensor(11.1935, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1750/8522\n",
      "tensor(11.1921, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1760/8522\n",
      "tensor(11.1925, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1770/8522\n",
      "tensor(11.1947, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1780/8522\n",
      "tensor(11.1912, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1790/8522\n",
      "tensor(11.1903, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1800/8522\n",
      "tensor(11.1887, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1810/8522\n",
      "tensor(11.1845, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1820/8522\n",
      "tensor(11.1905, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1830/8522\n",
      "tensor(11.1821, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1840/8522\n",
      "tensor(11.1815, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1850/8522\n",
      "tensor(11.1879, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1860/8522\n",
      "tensor(11.1904, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1870/8522\n",
      "tensor(11.1878, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1880/8522\n",
      "tensor(11.1882, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1890/8522\n",
      "tensor(11.1861, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1900/8522\n",
      "tensor(11.1842, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1910/8522\n",
      "tensor(11.1857, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1920/8522\n",
      "tensor(11.1857, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1930/8522\n",
      "tensor(11.1756, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1940/8522\n",
      "tensor(11.1819, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1950/8522\n",
      "tensor(11.1865, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1960/8522\n",
      "tensor(11.1842, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1970/8522\n",
      "tensor(11.1795, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1980/8522\n",
      "tensor(11.1809, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1990/8522\n",
      "tensor(11.1796, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2000/8522\n",
      "tensor(11.1797, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2010/8522\n",
      "tensor(11.1768, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2020/8522\n",
      "tensor(11.1827, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2030/8522\n",
      "tensor(11.1740, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2040/8522\n",
      "tensor(11.1836, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2050/8522\n",
      "tensor(11.1830, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2060/8522\n",
      "tensor(11.1826, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2070/8522\n",
      "tensor(11.1752, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2080/8522\n",
      "tensor(11.1812, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2090/8522\n",
      "tensor(11.1781, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2100/8522\n",
      "tensor(11.1835, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2110/8522\n",
      "tensor(11.1807, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2120/8522\n",
      "tensor(11.1821, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2130/8522\n",
      "tensor(11.1819, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2140/8522\n",
      "tensor(11.1741, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2150/8522\n",
      "tensor(11.1806, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2160/8522\n",
      "tensor(11.1784, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2170/8522\n",
      "tensor(11.1796, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2180/8522\n",
      "tensor(11.1787, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2190/8522\n",
      "tensor(11.1767, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2200/8522\n",
      "tensor(11.1795, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2210/8522\n",
      "tensor(11.1777, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2220/8522\n",
      "tensor(11.1776, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2230/8522\n",
      "tensor(11.1749, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2240/8522\n",
      "tensor(11.1765, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2250/8522\n",
      "tensor(11.1772, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2260/8522\n",
      "tensor(11.1772, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2270/8522\n",
      "tensor(11.1758, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2280/8522\n",
      "tensor(11.1782, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2290/8522\n",
      "tensor(11.1742, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2300/8522\n",
      "tensor(11.1768, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2310/8522\n",
      "tensor(11.1781, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2320/8522\n",
      "tensor(11.1777, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2330/8522\n",
      "tensor(11.1772, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2340/8522\n",
      "tensor(11.1770, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2350/8522\n",
      "tensor(11.1763, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2360/8522\n",
      "tensor(11.1760, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2370/8522\n",
      "tensor(11.1755, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2380/8522\n",
      "tensor(11.1769, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2390/8522\n",
      "tensor(11.1760, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2400/8522\n",
      "tensor(11.1755, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2410/8522\n",
      "tensor(11.1760, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2420/8522\n",
      "tensor(11.1762, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2430/8522\n",
      "tensor(11.1760, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2440/8522\n",
      "tensor(11.1762, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2450/8522\n",
      "tensor(11.1756, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2460/8522\n",
      "tensor(11.1755, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2470/8522\n",
      "tensor(11.1743, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2480/8522\n",
      "tensor(11.1751, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2490/8522\n",
      "tensor(11.1778, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2500/8522\n",
      "tensor(11.1745, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2510/8522\n",
      "tensor(11.1751, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2520/8522\n",
      "tensor(11.1743, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2530/8522\n",
      "tensor(11.1742, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2540/8522\n",
      "tensor(11.1735, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2550/8522\n",
      "tensor(11.1709, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2560/8522\n",
      "tensor(11.1747, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2570/8522\n",
      "tensor(11.1746, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2580/8522\n",
      "tensor(11.1726, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2590/8522\n",
      "tensor(11.1748, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2600/8522\n",
      "tensor(11.1705, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2610/8522\n",
      "tensor(11.1742, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2620/8522\n",
      "tensor(11.1732, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2630/8522\n",
      "tensor(11.1740, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2640/8522\n",
      "tensor(11.1713, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2650/8522\n",
      "tensor(11.1715, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2660/8522\n",
      "tensor(11.1714, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2670/8522\n",
      "tensor(11.1706, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2680/8522\n",
      "tensor(11.1721, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2690/8522\n",
      "tensor(11.1711, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2700/8522\n",
      "tensor(11.1717, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2710/8522\n",
      "tensor(11.1712, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2720/8522\n",
      "tensor(11.1707, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2730/8522\n",
      "tensor(11.1724, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2740/8522\n",
      "tensor(11.1727, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2750/8522\n",
      "tensor(11.1727, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2760/8522\n",
      "tensor(11.1699, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2770/8522\n",
      "tensor(11.1719, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2780/8522\n",
      "tensor(11.1721, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2790/8522\n",
      "tensor(11.1726, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2800/8522\n",
      "tensor(11.1726, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2810/8522\n",
      "tensor(11.1726, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2820/8522\n",
      "tensor(11.1726, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2830/8522\n",
      "tensor(11.1725, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2840/8522\n",
      "tensor(11.1721, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2850/8522\n",
      "tensor(11.1704, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2860/8522\n",
      "tensor(11.1724, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2870/8522\n",
      "tensor(11.1720, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2880/8522\n",
      "tensor(11.1716, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2890/8522\n",
      "tensor(11.1716, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2900/8522\n",
      "tensor(11.1718, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2910/8522\n",
      "tensor(11.1718, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2920/8522\n",
      "tensor(11.1715, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2930/8522\n",
      "tensor(11.1712, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2940/8522\n",
      "tensor(11.1710, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2950/8522\n",
      "tensor(11.1719, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2960/8522\n",
      "tensor(11.1708, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2970/8522\n",
      "tensor(11.1716, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2980/8522\n",
      "tensor(11.1703, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2990/8522\n",
      "tensor(11.1709, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3000/8522\n",
      "tensor(11.1713, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3010/8522\n",
      "tensor(11.1709, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3020/8522\n",
      "tensor(11.1711, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3030/8522\n",
      "tensor(11.1712, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3040/8522\n",
      "tensor(11.1705, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3050/8522\n",
      "tensor(11.1695, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3060/8522\n",
      "tensor(11.1703, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3070/8522\n",
      "tensor(11.1696, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3080/8522\n",
      "tensor(11.1713, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3090/8522\n",
      "tensor(11.1705, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3100/8522\n",
      "tensor(11.1698, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3110/8522\n",
      "tensor(11.1701, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3120/8522\n",
      "tensor(11.1700, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3130/8522\n",
      "tensor(11.1711, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3140/8522\n",
      "tensor(11.1703, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3150/8522\n",
      "tensor(11.1711, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3160/8522\n",
      "tensor(11.1708, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3170/8522\n",
      "tensor(11.1703, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3180/8522\n",
      "tensor(11.1705, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3190/8522\n",
      "tensor(11.1703, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3200/8522\n",
      "tensor(11.1702, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3210/8522\n",
      "tensor(11.1696, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3220/8522\n",
      "tensor(11.1701, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3230/8522\n",
      "tensor(11.1705, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3240/8522\n",
      "tensor(11.1703, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3250/8522\n",
      "tensor(11.1699, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3260/8522\n",
      "tensor(11.1699, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3270/8522\n",
      "tensor(11.1703, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3280/8522\n",
      "tensor(11.1701, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3290/8522\n",
      "tensor(11.1704, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3300/8522\n",
      "tensor(11.1696, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3310/8522\n",
      "tensor(11.1690, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3320/8522\n",
      "tensor(11.1702, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3330/8522\n",
      "tensor(11.1701, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3340/8522\n",
      "tensor(11.1699, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3350/8522\n",
      "tensor(11.1699, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3360/8522\n",
      "tensor(11.1698, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3370/8522\n",
      "tensor(11.1699, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3380/8522\n",
      "tensor(11.1700, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3390/8522\n",
      "tensor(11.1700, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3400/8522\n",
      "tensor(11.1699, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3410/8522\n",
      "tensor(11.1695, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3420/8522\n",
      "tensor(11.1695, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3430/8522\n",
      "tensor(11.1697, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3440/8522\n",
      "tensor(11.1694, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3450/8522\n",
      "tensor(11.1698, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3460/8522\n",
      "tensor(11.1695, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3470/8522\n",
      "tensor(11.1694, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3480/8522\n",
      "tensor(11.1694, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3490/8522\n",
      "tensor(11.1692, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3500/8522\n",
      "tensor(11.1695, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3510/8522\n",
      "tensor(11.1694, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3520/8522\n",
      "tensor(11.1691, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3530/8522\n",
      "tensor(11.1697, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3540/8522\n",
      "tensor(11.1689, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3550/8522\n",
      "tensor(11.1695, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3560/8522\n",
      "tensor(11.1691, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3570/8522\n",
      "tensor(11.1695, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3580/8522\n",
      "tensor(11.1691, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3590/8522\n",
      "tensor(11.1690, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3600/8522\n",
      "tensor(11.1689, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3610/8522\n",
      "tensor(11.1691, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3620/8522\n",
      "tensor(11.1692, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3630/8522\n",
      "tensor(11.1689, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3640/8522\n",
      "tensor(11.1687, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3650/8522\n",
      "tensor(11.1692, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3660/8522\n",
      "tensor(11.1687, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3670/8522\n",
      "tensor(11.1690, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3680/8522\n",
      "tensor(11.1686, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3690/8522\n",
      "tensor(11.1694, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3700/8522\n",
      "tensor(11.1694, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3710/8522\n",
      "tensor(11.1692, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3720/8522\n",
      "tensor(11.1688, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3730/8522\n",
      "tensor(11.1692, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3740/8522\n",
      "tensor(11.1692, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3750/8522\n",
      "tensor(11.1692, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3760/8522\n",
      "tensor(11.1690, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3770/8522\n",
      "tensor(11.1690, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3780/8522\n",
      "tensor(11.1690, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3790/8522\n",
      "tensor(11.1688, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3800/8522\n",
      "tensor(11.1689, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3810/8522\n",
      "tensor(11.1689, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3820/8522\n",
      "tensor(11.1690, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3830/8522\n",
      "tensor(11.1690, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3840/8522\n",
      "tensor(11.1691, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3850/8522\n",
      "tensor(11.1690, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3860/8522\n",
      "tensor(11.1691, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3870/8522\n",
      "tensor(11.1686, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3880/8522\n",
      "tensor(11.1690, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3890/8522\n",
      "tensor(11.1690, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3900/8522\n",
      "tensor(11.1689, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3910/8522\n",
      "tensor(11.1690, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3920/8522\n",
      "tensor(11.1689, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3930/8522\n",
      "tensor(11.1689, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3940/8522\n",
      "tensor(11.1689, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3950/8522\n",
      "tensor(11.1687, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3960/8522\n",
      "tensor(11.1685, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3970/8522\n",
      "tensor(11.1685, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3980/8522\n",
      "tensor(11.1686, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3990/8522\n",
      "tensor(11.1686, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4000/8522\n",
      "tensor(11.1687, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4010/8522\n",
      "tensor(11.1687, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4020/8522\n",
      "tensor(11.1686, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4030/8522\n",
      "tensor(11.1686, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4040/8522\n",
      "tensor(11.1687, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4050/8522\n",
      "tensor(11.1688, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4060/8522\n",
      "tensor(11.1688, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4070/8522\n",
      "tensor(11.1687, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4080/8522\n",
      "tensor(11.1687, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4090/8522\n",
      "tensor(11.1687, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4100/8522\n",
      "tensor(11.1687, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4110/8522\n",
      "tensor(11.1686, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4120/8522\n",
      "tensor(11.1686, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4130/8522\n",
      "tensor(11.1687, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4140/8522\n",
      "tensor(11.1687, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4150/8522\n",
      "tensor(11.1687, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4160/8522\n",
      "tensor(11.1686, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4170/8522\n",
      "tensor(11.1685, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4180/8522\n",
      "tensor(11.1686, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4190/8522\n",
      "tensor(11.1686, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4200/8522\n",
      "tensor(11.1685, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4210/8522\n",
      "tensor(11.1686, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4220/8522\n",
      "tensor(11.1686, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4230/8522\n",
      "tensor(11.1686, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4240/8522\n",
      "tensor(11.1686, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4250/8522\n",
      "tensor(11.1686, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4260/8522\n",
      "tensor(11.1685, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4270/8522\n",
      "tensor(11.1685, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4280/8522\n",
      "tensor(11.1686, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4290/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4300/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4310/8522\n",
      "tensor(11.1685, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4320/8522\n",
      "tensor(11.1686, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4330/8522\n",
      "tensor(11.1685, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4340/8522\n",
      "tensor(11.1685, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4350/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4360/8522\n",
      "tensor(11.1685, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4370/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4380/8522\n",
      "tensor(11.1685, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4390/8522\n",
      "tensor(11.1686, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4400/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4410/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4420/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4430/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4440/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4450/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4460/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4470/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4480/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4490/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4500/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4510/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4520/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4530/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4540/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4550/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4560/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4570/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4580/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4590/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4600/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4610/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4620/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4630/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4640/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4650/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4660/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4670/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4680/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4690/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4700/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4710/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4720/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4730/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4740/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4750/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4760/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4770/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4780/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4790/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4800/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4810/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4820/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4830/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4840/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4850/8522\n",
      "tensor(11.1684, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4860/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4870/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4880/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4890/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4900/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4910/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4920/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4930/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4940/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4950/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4960/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4970/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4980/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4990/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5000/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5010/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5020/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5030/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5040/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5050/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5060/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5070/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5080/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5090/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5100/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5110/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5120/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5130/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5140/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5150/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5160/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5170/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5180/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5190/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5200/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5210/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5220/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5230/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5240/8522\n",
      "tensor(11.1683, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5250/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5260/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5270/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5280/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5290/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5300/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5310/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5320/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5330/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5340/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5350/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5360/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5370/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5380/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5390/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5400/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5410/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5420/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5430/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5440/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5450/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5460/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5470/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5480/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5490/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5500/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5510/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5520/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5530/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5540/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5550/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5560/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5570/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5580/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5590/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5600/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5610/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5620/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5630/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5640/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5650/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5660/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5670/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5680/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5690/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5700/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5710/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5720/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5730/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5740/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5750/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5760/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5770/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5780/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5790/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5800/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5810/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5820/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5830/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5840/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5850/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5860/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5870/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5880/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5890/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5900/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5910/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5920/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5930/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5940/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5950/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5960/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5970/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5980/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5990/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6000/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6010/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6020/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6030/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6040/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6050/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6060/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6070/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6080/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6090/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6100/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6110/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6120/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6130/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6140/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6150/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6160/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6170/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6180/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6190/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6200/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6210/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6220/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6230/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6240/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6250/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6260/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6270/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6280/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6290/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6300/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6310/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6320/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6330/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6340/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6350/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6360/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6370/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6380/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6390/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6400/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6410/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6430/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6440/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6460/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6470/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6480/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6490/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6500/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6510/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6520/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6530/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6540/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6550/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6560/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6570/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6580/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6590/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6600/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6610/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6620/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6630/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6640/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6650/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6660/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6670/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6680/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6690/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6700/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6710/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6720/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6730/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6740/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6750/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6760/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6770/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6780/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6790/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6800/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6810/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6820/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6830/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6840/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6850/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6860/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6870/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6880/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6890/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6900/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6910/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6920/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6930/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6940/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6950/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6960/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6970/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6980/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6990/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7000/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7010/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7020/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7030/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7040/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7050/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7060/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7070/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7080/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7090/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7100/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7110/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7140/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7150/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7160/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7180/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7200/8522\n",
      "tensor(11.1682, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7530/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7540/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7550/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7560/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7570/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7580/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7590/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7600/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7610/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7620/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7630/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7640/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7650/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7660/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7670/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7680/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7690/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7700/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7710/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7720/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7730/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7740/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7750/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7760/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7770/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7780/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7790/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7800/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7810/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7820/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7830/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7840/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7850/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7860/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7870/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7880/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7890/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7900/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7910/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7920/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7930/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7940/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7950/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7960/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7970/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7980/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7990/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8000/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8010/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8020/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8030/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8040/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8050/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8060/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8070/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8080/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8090/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8100/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8110/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8140/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8150/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8160/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8180/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8200/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "========== Epoch 1 ==========\n",
      "Batch 1/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 10/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 20/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 30/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 40/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 50/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 60/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 70/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 80/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 90/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 100/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 110/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 140/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 150/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 160/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 180/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 200/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 530/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 540/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 550/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 560/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 570/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 580/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 590/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 600/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 610/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 620/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 630/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 640/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 650/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 660/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 670/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 680/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 690/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 700/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 710/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 720/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 730/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 740/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 750/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 760/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 770/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 780/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 790/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 800/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 810/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 820/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 830/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 840/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 850/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 860/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 870/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 880/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 890/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 900/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 910/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 920/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 930/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 940/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 950/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 960/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 970/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 980/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 990/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1000/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1010/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1020/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1030/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1040/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1050/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1060/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1070/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1080/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1090/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1100/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1110/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1140/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1150/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1160/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1180/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1200/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1530/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1540/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1550/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1560/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1570/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1580/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1590/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1600/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1610/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1620/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1630/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1640/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1650/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1660/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1670/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1680/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1690/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1700/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1710/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1720/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1730/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1740/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1750/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1760/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1770/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1780/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1790/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1800/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1810/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1820/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1830/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1840/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1850/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1860/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1870/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1880/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1890/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1900/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1910/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1920/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1930/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1940/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1950/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1960/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1970/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1980/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1990/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2000/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2010/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2020/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2030/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2040/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2050/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2060/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2070/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2080/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2090/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2100/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2110/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2140/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2150/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2160/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2180/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2200/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2530/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2540/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2550/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2560/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2570/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2580/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2590/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2600/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2610/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2620/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2630/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2640/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2650/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2660/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2670/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2680/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2690/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2700/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2710/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2720/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2730/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2740/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2750/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2760/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2770/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2780/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2790/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2800/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2810/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2820/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2830/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2840/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2850/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2860/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2870/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2880/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2890/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2900/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2910/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2920/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2930/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2940/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2950/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2960/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2970/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2980/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2990/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3000/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3010/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3020/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3030/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3040/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3050/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3060/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3070/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3080/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3090/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3100/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3110/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3140/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3150/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3160/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3180/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3200/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3530/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3540/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3550/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3560/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3570/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3580/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3590/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3600/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3610/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3620/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3630/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3640/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3650/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3660/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3670/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3680/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3690/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3700/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3710/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3720/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3730/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3740/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3750/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3760/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3770/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3780/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3790/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3800/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3810/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3820/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3830/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3840/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3850/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3860/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3870/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3880/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3890/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3900/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3910/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3920/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3930/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3940/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3950/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3960/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3970/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3980/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3990/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4000/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4010/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4020/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4030/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4040/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4050/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4060/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4070/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4080/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4090/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4100/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4110/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4140/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4150/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4160/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4180/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4200/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4530/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4540/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4550/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4560/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4570/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4580/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4590/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4600/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4610/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4620/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4630/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4640/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4650/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4660/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4670/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4680/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4690/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4700/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4710/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4720/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4730/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4740/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4750/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4760/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4770/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4780/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4790/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4800/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4810/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4820/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4830/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4840/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4850/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4860/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4870/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4880/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4890/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4900/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4910/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4920/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4930/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4940/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4950/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4960/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4970/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4980/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4990/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5000/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5010/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5020/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5030/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5040/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5050/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5060/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5070/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5080/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5090/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5100/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5110/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5140/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5150/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5160/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5180/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5200/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5530/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5540/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5550/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5560/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5570/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5580/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5590/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5600/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5610/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5620/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5630/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5640/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5650/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5660/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5670/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5680/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5690/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5700/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5710/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5720/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5730/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5740/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5750/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5760/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5770/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5780/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5790/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5800/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5810/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5820/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5830/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5840/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5850/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5860/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5870/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5880/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5890/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5900/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5910/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5920/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5930/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5940/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5950/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5960/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5970/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5980/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5990/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6000/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6010/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6020/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6030/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6040/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6050/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6060/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6070/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6080/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6090/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6100/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6110/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6140/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6150/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6160/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6180/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6200/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6530/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6540/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6550/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6560/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6570/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6580/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6590/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6600/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6610/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6620/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6630/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6640/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6650/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6660/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6670/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6680/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6690/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6700/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6710/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6720/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6730/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6740/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6750/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6760/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6770/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6780/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6790/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6800/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6810/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6820/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6830/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6840/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6850/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6860/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6870/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6880/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6890/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6900/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6910/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6920/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6930/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6940/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6950/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6960/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6970/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6980/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6990/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7000/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7010/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7020/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7030/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7040/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7050/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7060/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7070/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7080/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7090/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7100/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7110/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7140/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7150/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7160/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7180/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7200/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7530/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7540/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7550/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7560/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7570/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7580/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7590/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7600/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7610/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7620/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7630/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7640/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7650/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7660/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7670/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7680/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7690/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7700/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7710/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7720/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7730/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7740/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7750/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7760/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7770/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7780/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7790/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7800/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7810/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7820/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7830/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7840/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7850/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7860/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7870/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7880/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7890/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7900/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7910/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7920/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7930/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7940/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7950/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7960/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7970/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7980/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7990/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8000/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8010/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8020/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8030/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8040/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8050/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8060/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8070/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8080/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8090/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8100/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8110/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8140/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8150/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8160/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8180/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8200/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "========== Epoch 2 ==========\n",
      "Batch 1/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 10/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 20/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 30/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 40/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 50/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 60/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 70/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 80/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 90/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 100/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 110/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 140/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 150/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 160/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 180/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 200/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 530/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 540/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 550/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 560/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 570/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 580/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 590/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 600/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 610/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 620/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 630/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 640/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 650/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 660/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 670/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 680/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 690/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 700/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 710/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 720/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 730/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 740/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 750/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 760/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 770/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 780/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 790/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 800/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 810/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 820/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 830/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 840/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 850/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 860/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 870/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 880/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 890/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 900/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 910/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 920/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 930/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 940/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 950/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 960/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 970/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 980/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 990/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1000/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1010/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1020/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1030/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1040/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1050/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1060/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1070/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1080/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1090/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1100/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1110/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1140/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1150/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1160/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1180/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1200/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1530/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1540/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1550/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1560/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1570/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1580/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1590/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1600/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1610/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1620/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1630/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1640/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1650/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1660/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1670/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1680/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1690/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1700/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1710/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1720/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1730/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1740/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1750/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1760/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1770/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1780/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1790/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1800/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1810/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1820/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1830/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1840/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1850/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1860/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1870/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1880/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1890/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1900/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1910/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1920/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1930/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1940/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1950/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1960/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1970/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1980/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 1990/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2000/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2010/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2020/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2030/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2040/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2050/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2060/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2070/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2080/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2090/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2100/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2110/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2140/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2150/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2160/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2180/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2200/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2530/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2540/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2550/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2560/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2570/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2580/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2590/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2600/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2610/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2620/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2630/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2640/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2650/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2660/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2670/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2680/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2690/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2700/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2710/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2720/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2730/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2740/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2750/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2760/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2770/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2780/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2790/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2800/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2810/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2820/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2830/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2840/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2850/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2860/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2870/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2880/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2890/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2900/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2910/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2920/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2930/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2940/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2950/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2960/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2970/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2980/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 2990/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3000/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3010/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3020/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3030/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3040/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3050/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3060/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3070/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3080/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3090/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3100/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3110/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3140/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3150/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3160/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3180/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3200/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3530/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3540/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3550/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3560/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3570/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3580/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3590/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3600/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3610/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3620/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3630/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3640/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3650/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3660/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3670/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3680/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3690/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3700/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3710/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3720/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3730/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3740/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3750/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3760/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3770/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3780/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3790/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3800/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3810/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3820/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3830/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3840/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3850/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3860/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3870/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3880/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3890/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3900/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3910/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3920/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3930/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3940/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3950/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3960/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3970/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3980/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 3990/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4000/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4010/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4020/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4030/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4040/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4050/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4060/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4070/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4080/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4090/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4100/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4110/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4140/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4150/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4160/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4180/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4200/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4530/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4540/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4550/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4560/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4570/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4580/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4590/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4600/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4610/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4620/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4630/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4640/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4650/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4660/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4670/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4680/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4690/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4700/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4710/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4720/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4730/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4740/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4750/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4760/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4770/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4780/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4790/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4800/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4810/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4820/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4830/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4840/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4850/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4860/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4870/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4880/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4890/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4900/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4910/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4920/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4930/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4940/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4950/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4960/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4970/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4980/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 4990/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5000/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5010/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5020/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5030/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5040/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5050/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5060/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5070/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5080/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5090/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5100/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5110/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5140/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5150/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5160/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5180/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5200/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5530/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5540/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5550/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5560/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5570/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5580/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5590/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5600/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5610/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5620/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5630/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5640/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5650/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5660/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5670/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5680/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5690/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5700/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5710/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5720/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5730/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5740/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5750/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5760/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5770/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5780/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5790/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5800/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5810/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5820/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5830/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5840/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5850/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5860/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5870/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5880/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5890/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5900/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5910/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5920/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5930/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5940/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5950/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5960/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5970/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5980/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 5990/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6000/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6010/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6020/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6030/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6040/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6050/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6060/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6070/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6080/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6090/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6100/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6110/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6140/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6150/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6160/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6180/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6200/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6530/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6540/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6550/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6560/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6570/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6580/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6590/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6600/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6610/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6620/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6630/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6640/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6650/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6660/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6670/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6680/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6690/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6700/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6710/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6720/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6730/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6740/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6750/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6760/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6770/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6780/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6790/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6800/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6810/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6820/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6830/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6840/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6850/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6860/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6870/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6880/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6890/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6900/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6910/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6920/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6930/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6940/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6950/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6960/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6970/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6980/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 6990/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7000/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7010/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7020/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7030/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7040/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7050/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7060/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7070/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7080/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7090/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7100/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7110/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7140/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7150/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7160/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7180/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7200/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7530/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7540/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7550/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7560/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7570/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7580/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7590/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7600/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7610/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7620/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7630/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7640/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7650/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7660/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7670/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7680/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7690/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7700/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7710/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7720/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7730/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7740/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7750/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7760/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7770/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7780/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7790/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7800/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7810/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7820/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7830/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7840/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7850/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7860/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7870/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7880/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7890/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7900/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7910/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7920/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7930/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7940/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7950/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7960/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7970/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7980/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 7990/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8000/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8010/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8020/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8030/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8040/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8050/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8060/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8070/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8080/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8090/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8100/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8110/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8120/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8130/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8140/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8150/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8160/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8170/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8180/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8190/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8200/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8210/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8220/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8230/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8240/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8250/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8260/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8270/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8280/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8290/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8300/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8310/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8320/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8330/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8340/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8350/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8360/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8370/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8380/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8390/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8400/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8410/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8420/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8430/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8440/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8450/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8460/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8470/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8480/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8490/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8500/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8510/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "Batch 8520/8522\n",
      "tensor(11.1681, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(voc_size=batcher.voc_size, embedding_dim=300, window_size=batcher.window_size, batch_size=batcher.batch_size)\n",
    "model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=100)\n",
    "\n",
    "for epoch in [0, 1, 2]:\n",
    "    print('========== Epoch {} =========='.format(epoch))\n",
    "    total_loss = 0\n",
    "    i = 1\n",
    "    N = int(len(batcher.words)//BATCH_SIZE)\n",
    "    for context, target in batcher.generator():\n",
    "        model.train()\n",
    "        context = torch.tensor(context).to(device='cuda')\n",
    "        target = torch.tensor(target).to(device='cuda')\n",
    "        \n",
    "        log_probs = model(context)\n",
    "        loss = loss_function(log_probs, target)\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if i%10==0 or i==1:\n",
    "            print('Batch {}/{}'.format(i, N))\n",
    "            print(loss)\n",
    "        i += 1\n",
    "        losses.append(loss)\n",
    "        #model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

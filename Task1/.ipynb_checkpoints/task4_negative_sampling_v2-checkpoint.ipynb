{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1.4: Negative sampling (15 points)\n",
    "\n",
    "You may have noticed that word2vec is really slow to train. Especially with big (> 50 000) vocabularies. Negative sampling is the solution.\n",
    "\n",
    "The task is to implement word2vec with negative sampling.\n",
    "\n",
    "This is what was discussed in Stanford lecture. The main idea is in the formula:\n",
    "\n",
    "$$ L = \\log\\sigma(u^T_o \\cdot u_c) + \\sum^k_{i=1} \\mathbb{E}_{j \\sim P(w)}[\\log\\sigma(-u^T_j \\cdot u_c)]$$\n",
    "\n",
    "Where $\\sigma$ - sigmoid function, $u_c$ - central word vector, $u_o$ - context (outside of the window) word vector, $u_j$ - vector or word with index $j$.\n",
    "\n",
    "The first term calculates the similarity between positive examples (word from one window)\n",
    "\n",
    "The second term is responsible for negative samples. $k$ is a hyperparameter - the number of negatives to sample.\n",
    "$\\mathbb{E}_{j \\sim P(w)}$\n",
    "means that $j$ is distributed accordingly to unigram distribution.\n",
    "\n",
    "Thus, it is only required to calculate the similarity between positive samples and some other negatives. Not across all the vocabulary.\n",
    "\n",
    "Useful links:\n",
    "1. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "1. [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "gc.collect()\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "len(STOP_WORDS)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "import numpy\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print (torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batcher:\n",
    "    def __init__(self, max_len, window_size, corpus_path, min_freq, max_freq, max_voc_size, batch_size):\n",
    "        self.corpus_path = corpus_path\n",
    "        self.window_size = window_size\n",
    "        self.min_freq = min_freq\n",
    "        self.max_freq = max_freq\n",
    "        self.max_voc_size = max_voc_size\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "        self.words = None\n",
    "        self.word2index = None\n",
    "        self.index2word = None\n",
    "        self.freq = None\n",
    "        self.voc = None\n",
    "        self.voc_size = None\n",
    "        self.corpus = None\n",
    "        self.corpus_size = None\n",
    "        \n",
    "        \n",
    "    def read_data(self, S):\n",
    "        if S == None:\n",
    "            with open(self.corpus_path, 'r') as f:\n",
    "                S = f.read()\n",
    "            if S!=None:\n",
    "                S = S.lower()[: self.max_len]\n",
    "        print('Len of S = ', len(S))\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        S = regex.sub(' ', S)\n",
    "        words_raw = list(S.split())\n",
    "        print(len(words_raw))\n",
    "        words = []\n",
    "        for word in words_raw:\n",
    "            if word in STOP_WORDS:\n",
    "                pass\n",
    "            else:\n",
    "                words.append(word)\n",
    "\n",
    "        print('Size of words = ', len(words))\n",
    "        counter = Counter(words)\n",
    "        print('Size of counter = ', len(counter))\n",
    "        if self.min_freq != None:\n",
    "            counter = {x : counter[x] for x in counter if counter[x] >= self.min_freq}\n",
    "        print('Size of counter after min_freq = ', len(counter))\n",
    "        if self.max_freq != None:\n",
    "            counter = {x : counter[x] for x in counter if counter[x] <= self.max_freq}\n",
    "        print('Size of counter after max_freq = ', len(counter))\n",
    "        counter = Counter(counter)\n",
    "\n",
    "        freq = dict(counter.most_common(self.max_voc_size))\n",
    "        voc = set(freq)\n",
    "        \n",
    "        unk = set(words).difference(voc)\n",
    "        print('Size of freq dict = ', len(voc))\n",
    "        print('Number of vocabulary words = ', len(voc))\n",
    "        print('Number of unknown words = ', len(unk))\n",
    "\n",
    "        words = ['UNK' if word in unk else word for word in words]        \n",
    "        if len(words)%self.batch_size == 0:\n",
    "            padding = self.window_size\n",
    "        else:\n",
    "            padding = self.batch_size - len(words)%self.batch_size + self.window_size\n",
    "            \n",
    "        words = ['PAD']*self.window_size + words + ['PAD']*padding\n",
    "        unique_words = list(set(words))\n",
    "        print('Size of corpus = ', len(words))\n",
    "        print('Size of vocabulary = ', len(unique_words))\n",
    "        self.word2index = {k: v for v, k in enumerate(unique_words)}\n",
    "        self.index2word = {v: k for v, k in enumerate(unique_words)}\n",
    "        words = [self.word2index[word] for word in words]\n",
    "        self.freq = Counter(words)\n",
    "        self.voc = set(self.freq)\n",
    "        self.voc_size = len(self.voc)\n",
    "        self.corpus = words\n",
    "        self.corpus_size = len(words)\n",
    "    \n",
    "    def generator(self):\n",
    "        i = self.window_size\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        while i < self.corpus_size-self.window_size:\n",
    "            if len(x_batch)==self.batch_size:\n",
    "                x_batch = []\n",
    "                y_batch = []\n",
    "                \n",
    "            x = self.corpus[i-self.window_size: i] + self.corpus[i+1: i+self.window_size+1]\n",
    "#             y = [0]*self.voc_size\n",
    "#             y[self.corpus[i]] = 1\n",
    "            y = [self.corpus[i]]\n",
    "            x_batch.append(x)\n",
    "            y_batch.append(y)\n",
    "            i += 1\n",
    "            if len(x_batch)==self.batch_size:\n",
    "                yield np.array(x_batch), np.array(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of S =  10000000\n",
      "1706282\n",
      "Size of words =  1090922\n",
      "Size of counter =  70835\n",
      "Size of counter after min_freq =  19359\n",
      "Size of counter after max_freq =  19359\n",
      "Size of freq dict =  19359\n",
      "Number of vocabulary words =  19359\n",
      "Number of unknown words =  51476\n",
      "Size of corpus =  1090932\n",
      "Size of vocabulary =  19361\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "MAX_LEN = 10000000\n",
    "batcher = Batcher(max_len=MAX_LEN, window_size=2, corpus_path='text8', min_freq=5, max_freq=None, max_voc_size=10000000, batch_size=BATCH_SIZE)\n",
    "batcher.read_data(S=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4) (8, 1)\n"
     ]
    }
   ],
   "source": [
    "for x, y in batcher.generator():\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism PAD\n",
      "anarchism PAD\n",
      "anarchism originated\n",
      "anarchism term\n",
      "originated PAD\n",
      "originated anarchism\n",
      "originated term\n",
      "originated abuse\n",
      "term anarchism\n",
      "term originated\n",
      "term abuse\n",
      "term first\n",
      "abuse originated\n",
      "abuse term\n",
      "abuse first\n",
      "abuse used\n",
      "first term\n",
      "first abuse\n",
      "first used\n",
      "first early\n",
      "used abuse\n",
      "used first\n",
      "used early\n",
      "used working\n",
      "early first\n",
      "early used\n",
      "early working\n",
      "early class\n",
      "working used\n",
      "working early\n",
      "working class\n",
      "working radicals\n"
     ]
    }
   ],
   "source": [
    "for x, y in batcher.generator():\n",
    "    for i in range(x.shape[0]):\n",
    "        target_word = y[i][0]\n",
    "        for j in range(x.shape[1]):\n",
    "            context_word = x[i][j]\n",
    "            print(batcher.index2word[target_word], batcher.index2word[context_word])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, voc_size, embedding_dim, window_size, batch_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embedding1 = nn.Embedding(voc_size, embedding_dim)\n",
    "        self.embedding2 = nn.Embedding(voc_size, embedding_dim)\n",
    "        \n",
    "    def forward(self, target_word, context_word):\n",
    "\n",
    "        target_word = torch.tensor(target_word).to(device='cuda')\n",
    "        context_word = torch.tensor(context_word).to(device='cuda')\n",
    "        \n",
    "        target_emb = self.embedding1(target_word)\n",
    "        context_emb = self.embedding2(context_word)\n",
    "        \n",
    "        z1 = torch.mul(target_emb, context_emb)\n",
    "        z2 = torch.sum(z1)\n",
    "        pos_loss = F.logsigmoid(z2)\n",
    "        \n",
    "        neg_loss = 0\n",
    "        for i in range(5):\n",
    "            negative_word = torch.tensor(numpy.random.choice(batcher.corpus)).to(device='cuda')\n",
    "            negative_emb = self.embedding2(negative_word)\n",
    "            z4 = torch.mul(target_emb, negative_emb)\n",
    "            z5 = torch.sum(z4)\n",
    "            z6 = F.logsigmoid(-z5)\n",
    "            neg_loss += z6\n",
    "\n",
    "        print('target_emb shape : ', target_emb.shape)\n",
    "        print('context_emb shape : ', context_emb.shape)\n",
    "        print('negative_emb shape : ', negative_emb.shape)\n",
    "        print('z1 shape : ', z1.shape)\n",
    "        print('z2 : ', z2)\n",
    "        \n",
    "        return -(pos_loss + neg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Epoch 0 ==========\n",
      "Batch 1 loss : 8.554247856140137\n",
      "Batch 2 loss : 5.681187629699707\n",
      "Batch 3 loss : 7.376816749572754\n",
      "Batch 4 loss : 6.6656365394592285\n",
      "Batch 5 loss : 7.099233627319336\n",
      "Batch 6 loss : 4.026013374328613\n",
      "Batch 7 loss : 9.054348945617676\n",
      "Batch 8 loss : 7.0196990966796875\n",
      "Batch 9 loss : 3.605301856994629\n",
      "Batch 10 loss : 4.993880748748779\n",
      "Batch 11 loss : 7.086320877075195\n",
      "Batch 12 loss : 8.006959915161133\n",
      "Batch 13 loss : 7.145155429840088\n",
      "Batch 14 loss : 3.966289520263672\n",
      "Batch 15 loss : 5.272380352020264\n",
      "Batch 16 loss : 4.349819660186768\n",
      "Batch 17 loss : 6.3697991371154785\n",
      "Batch 18 loss : 6.144553184509277\n",
      "Batch 19 loss : 5.561038970947266\n",
      "Batch 20 loss : 6.398571014404297\n",
      "Batch 21 loss : 3.7784817218780518\n",
      "Batch 22 loss : 8.237434387207031\n",
      "Batch 23 loss : 4.421940326690674\n",
      "Batch 24 loss : 4.251057147979736\n",
      "Batch 25 loss : 7.601797103881836\n",
      "Batch 26 loss : 5.603153228759766\n",
      "Batch 27 loss : 8.381185531616211\n",
      "Batch 28 loss : 4.720135688781738\n",
      "Batch 29 loss : 8.252762794494629\n",
      "Batch 30 loss : 6.179993629455566\n",
      "Batch 31 loss : 4.164640426635742\n",
      "Batch 32 loss : 5.58513069152832\n",
      "Batch 33 loss : 3.727710247039795\n",
      "Batch 34 loss : 8.266446113586426\n",
      "Batch 35 loss : 7.238832950592041\n",
      "Batch 36 loss : 3.9570586681365967\n",
      "Batch 37 loss : 6.811378002166748\n",
      "Batch 38 loss : 6.921115875244141\n",
      "Batch 39 loss : 4.858826637268066\n",
      "Batch 40 loss : 6.640243053436279\n",
      "Batch 41 loss : 5.990644454956055\n",
      "Batch 42 loss : 6.260745525360107\n",
      "Batch 43 loss : 5.594740390777588\n",
      "Batch 44 loss : 6.923793792724609\n",
      "Batch 45 loss : 6.073060035705566\n",
      "Batch 46 loss : 3.142587900161743\n",
      "Batch 47 loss : 7.523199081420898\n",
      "Batch 48 loss : 8.210685729980469\n",
      "Batch 49 loss : 7.444153308868408\n",
      "Batch 50 loss : 6.209444999694824\n",
      "Batch 51 loss : 4.983386516571045\n",
      "Batch 52 loss : 5.787632942199707\n",
      "Batch 53 loss : 6.378133773803711\n",
      "Batch 54 loss : 7.186987400054932\n",
      "Batch 55 loss : 4.550420761108398\n",
      "Batch 56 loss : 5.8682169914245605\n",
      "Batch 57 loss : 7.445719242095947\n",
      "Batch 58 loss : 5.463630676269531\n",
      "Batch 59 loss : 5.075556755065918\n",
      "Batch 60 loss : 6.857640266418457\n",
      "Batch 61 loss : 5.717297077178955\n",
      "Batch 62 loss : 5.136255264282227\n",
      "Batch 63 loss : 4.480870723724365\n",
      "Batch 64 loss : 6.004735946655273\n",
      "Batch 65 loss : 6.838244438171387\n",
      "Batch 66 loss : 4.093430995941162\n",
      "Batch 67 loss : 7.594982147216797\n",
      "Batch 68 loss : 7.113779544830322\n",
      "Batch 69 loss : 8.679058074951172\n",
      "Batch 70 loss : 6.318682670593262\n",
      "Batch 71 loss : 4.995631217956543\n",
      "Batch 72 loss : 5.583489418029785\n",
      "Batch 73 loss : 5.2967424392700195\n",
      "Batch 74 loss : 5.985228061676025\n",
      "Batch 75 loss : 2.232581377029419\n",
      "Batch 76 loss : 4.395612716674805\n",
      "Batch 77 loss : 3.429600954055786\n",
      "Batch 78 loss : 5.302127838134766\n",
      "Batch 79 loss : 5.548864364624023\n",
      "Batch 80 loss : 3.259751319885254\n",
      "Batch 81 loss : 3.6231839656829834\n",
      "Batch 82 loss : 6.778946876525879\n",
      "Batch 83 loss : 3.951756477355957\n",
      "Batch 84 loss : 3.420081615447998\n",
      "Batch 85 loss : 6.208624839782715\n",
      "Batch 86 loss : 5.578175067901611\n",
      "Batch 87 loss : 4.5305914878845215\n",
      "Batch 88 loss : 7.797656536102295\n",
      "Batch 89 loss : 3.9919464588165283\n",
      "Batch 90 loss : 6.574377059936523\n",
      "Batch 91 loss : 6.5069355964660645\n",
      "Batch 92 loss : 8.187226295471191\n",
      "Batch 93 loss : 4.491937160491943\n",
      "Batch 94 loss : 5.585093021392822\n",
      "Batch 95 loss : 6.311226844787598\n",
      "Batch 96 loss : 1.6375162601470947\n",
      "Batch 97 loss : 4.837408065795898\n",
      "Batch 98 loss : 1.6339786052703857\n",
      "Batch 99 loss : 6.981541633605957\n",
      "Batch 100 loss : 4.690842628479004\n",
      "Batch 101 loss : 5.901648044586182\n",
      "Batch 102 loss : 6.248394966125488\n",
      "Batch 103 loss : 5.303192138671875\n",
      "Batch 104 loss : 2.992034673690796\n",
      "Batch 105 loss : 1.4729218482971191\n",
      "Batch 106 loss : 5.76896333694458\n",
      "Batch 107 loss : 8.19598388671875\n",
      "Batch 108 loss : 5.096065044403076\n",
      "Batch 109 loss : 4.065927505493164\n",
      "Batch 110 loss : 7.131829261779785\n",
      "Batch 111 loss : 5.650390625\n",
      "Batch 112 loss : 4.944677829742432\n",
      "Batch 113 loss : 7.331990718841553\n",
      "Batch 114 loss : 5.621108055114746\n",
      "Batch 115 loss : 7.504694938659668\n",
      "Batch 116 loss : 5.863827228546143\n",
      "Batch 117 loss : 4.5350751876831055\n",
      "Batch 118 loss : 5.3302202224731445\n",
      "Batch 119 loss : 5.613419532775879\n",
      "Batch 120 loss : 7.939416408538818\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-095b17dbc658>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;31m#print(batcher.index2word[target_word], batcher.index2word[context_word])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-159-8e05bdb2879a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, target_word, context_word)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtarget_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mcontext_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### ReduceLROnPlateau\n",
    "\n",
    "losses = []\n",
    "#loss_function = nn.NLLLoss()\n",
    "model = CBOW(voc_size=batcher.voc_size, embedding_dim=256, window_size=batcher.window_size, batch_size=batcher.batch_size)\n",
    "model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer = optimizer, \\\n",
    "                                 mode = 'min', \\\n",
    "                                 factor = 0.5, \\\n",
    "                                 threshold = 0.001 \\\n",
    "                                )\n",
    "\n",
    "for epoch in [0, 1, 2]:\n",
    "    print('========== Epoch {} =========='.format(epoch))\n",
    "    total_loss = 0\n",
    "    i = 1\n",
    "    N = int(len(batcher.corpus)//BATCH_SIZE)\n",
    "    for context, target in batcher.generator():\n",
    "        batch_loss = 0\n",
    "        model.train()\n",
    "#         context = torch.tensor(context).to(device='cuda')\n",
    "#         target = torch.tensor(target).to(device='cuda')\n",
    "        for k in range(context.shape[0]):\n",
    "            target_word = target[k][0]\n",
    "            for j in range(context.shape[1]):\n",
    "                context_word = context[k][j]\n",
    "                #print(batcher.index2word[target_word], batcher.index2word[context_word])\n",
    "        \n",
    "                loss = model(target_word, context_word)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                #print(loss)\n",
    "                batch_loss += loss\n",
    "                #print('\\n')\n",
    "        print('Batch {} loss : {}'.format(i, batch_loss/(BATCH_SIZE*batcher.window_size*2)))\n",
    "        losses.append(batch_loss)\n",
    "        i += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

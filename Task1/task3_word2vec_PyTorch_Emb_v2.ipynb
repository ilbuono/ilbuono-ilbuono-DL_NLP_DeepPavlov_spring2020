{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1.3: Naive word2vec (40 points)\n",
    "\n",
    "This task can be formulated very simply. Follow this [paper](https://arxiv.org/pdf/1411.2738.pdf) and implement word2vec like a two-layer neural network with matrices $W$ and $W'$. One matrix projects words to low-dimensional 'hidden' space and the other - back to high-dimensional vocabulary space.\n",
    "\n",
    "![word2vec](https://i.stack.imgur.com/6eVXZ.jpg)\n",
    "\n",
    "You can use TensorFlow/PyTorch and code from your previous task.\n",
    "\n",
    "## Results of this task: (30 points)\n",
    " * trained word vectors (mention somewhere, how long it took to train)\n",
    " * plotted loss (so we can see that it has converged)\n",
    " * function to map token to corresponding word vector\n",
    " * beautiful visualizations (PCE, T-SNE), you can use TensorBoard and play with your vectors in 3D (don't forget to add screenshots to the task)\n",
    "\n",
    "## Extra questions: (10 points)\n",
    " * Intrinsic evaluation: you can find datasets [here](http://download.tensorflow.org/data/questions-words.txt)\n",
    " * Extrinsic evaluation: you can use [these](https://medium.com/@dataturks/rare-text-classification-open-datasets-9d340c8c508e)\n",
    "\n",
    "Also, you can find any other datasets for quantitative evaluation.\n",
    "\n",
    "Again. It is **highly recommended** to read this [paper](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "\n",
    "Example of visualization in tensorboard:\n",
    "https://projector.tensorflow.org\n",
    "\n",
    "Example of 2D visualisation:\n",
    "\n",
    "![2dword2vec](https://www.tensorflow.org/images/tsne.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "gc.collect()\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "print(len(STOP_WORDS))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create own batcher with batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batcher:\n",
    "    def __init__(self, max_len, window_size, corpus_path, min_freq, max_freq, max_voc_size, batch_size):\n",
    "        self.corpus_path = corpus_path\n",
    "        self.window_size = window_size\n",
    "        self.min_freq = min_freq\n",
    "        self.max_freq = max_freq\n",
    "        self.max_voc_size = max_voc_size\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "        self.words = None\n",
    "        self.word2index = None\n",
    "        self.index2word = None\n",
    "        self.freq = None\n",
    "        self.voc = None\n",
    "        self.voc_size = None\n",
    "        self.corpus = None\n",
    "        self.corpus_size = None\n",
    "        \n",
    "        \n",
    "    def read_data(self, S):\n",
    "        if S == None:\n",
    "            with open(self.corpus_path, 'r') as f:\n",
    "                S = f.read()\n",
    "            if S!=None:\n",
    "                S = S.lower()[: self.max_len]\n",
    "        print('Len of S = ', len(S))\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        S = regex.sub(' ', S)\n",
    "        words_raw = list(S.split())\n",
    "        words = []\n",
    "        for word in words_raw:\n",
    "            if word in STOP_WORDS:\n",
    "                pass\n",
    "            else:\n",
    "                words.append(word)\n",
    "        \n",
    "        print('Size of words = ', len(words))\n",
    "        counter = Counter(words)\n",
    "        print('Size of counter = ', len(counter))\n",
    "        if self.min_freq != None:\n",
    "            counter = {x : counter[x] for x in counter if counter[x] >= self.min_freq}\n",
    "        print('Size of counter after min_freq = ', len(counter))\n",
    "        if self.max_freq != None:\n",
    "            counter = {x : counter[x] for x in counter if counter[x] <= self.max_freq}\n",
    "        print('Size of counter after max_freq = ', len(counter))\n",
    "        counter = Counter(counter)\n",
    "\n",
    "        freq = dict(counter.most_common(self.max_voc_size))\n",
    "        voc = set(freq)\n",
    "        \n",
    "        unk = set(words).difference(voc)\n",
    "        print('Size of freq dict = ', len(voc))\n",
    "        print('Number of vocabulary words = ', len(voc))\n",
    "        print('Number of unknown words = ', len(unk))\n",
    "\n",
    "        words = ['UNK' if word in unk else word for word in words]        \n",
    "        if len(words)%self.batch_size == 0:\n",
    "            padding = self.window_size\n",
    "        else:\n",
    "            padding = self.batch_size - len(words)%self.batch_size + self.window_size\n",
    "            \n",
    "        words = ['PAD']*self.window_size + words + ['PAD']*padding\n",
    "        unique_words = list(set(words))\n",
    "        print('Size of corpus = ', len(words))\n",
    "        print('Size of vocabulary = ', len(unique_words))\n",
    "        self.word2index = {k: v for v, k in enumerate(unique_words)}\n",
    "        self.index2word = {v: k for v, k in enumerate(unique_words)}\n",
    "        words = [self.word2index[word] for word in words]\n",
    "        self.freq = Counter(words)\n",
    "        self.voc = set(self.freq)\n",
    "        self.voc_size = len(self.voc)\n",
    "        self.corpus = words\n",
    "        self.corpus_size = len(words)\n",
    "    \n",
    "    def generator(self):\n",
    "        i = self.window_size\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        while i < self.corpus_size-self.window_size:\n",
    "            if len(x_batch)==self.batch_size:\n",
    "                x_batch = []\n",
    "                y_batch = []\n",
    "                \n",
    "            x = self.corpus[i-self.window_size: i] + self.corpus[i+1: i+self.window_size+1]\n",
    "            y = [0]*self.voc_size\n",
    "            y[self.corpus[i]] = 1\n",
    "            x_batch.append(x)\n",
    "            y_batch.append(y)\n",
    "            i += 1\n",
    "            if len(x_batch)==self.batch_size:\n",
    "                yield np.array(x_batch), np.array(y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Batcher with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of S =  100000000\n",
      "17005207\n",
      "10890638\n",
      "Size of words =  10890638\n",
      "Size of counter =  253702\n",
      "Size of counter after min_freq =  71140\n",
      "Size of counter after max_freq =  71140\n",
      "Size of freq dict =  71140\n",
      "Number of vocabulary words =  71140\n",
      "Number of unknown words =  182562\n",
      "Size of corpus =  10890692\n",
      "Size of vocabulary =  71142\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 1000000000\n",
    "batcher = Batcher(max_len=MAX_LEN, window_size=2, corpus_path='text8', min_freq=5, max_freq=None, max_voc_size=10000000, batch_size=BATCH_SIZE)\n",
    "batcher.read_data(S=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check dimentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 4) (64, 71142)\n"
     ]
    }
   ],
   "source": [
    "for x, y in batcher.generator():\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check value of the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_words = []\n",
    "for i in range(BATCH_SIZE):\n",
    "    line = []\n",
    "    for j in range(batcher.window_size*2):\n",
    "        line.append(batcher.index2word[x[i, j]])\n",
    "    x_words.append(line)\n",
    "x_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(BATCH_SIZE):\n",
    "    print(batcher.index2word[list(y[i]).index(1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CBOW class using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print (torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, voc_size, embedding_dim, window_size, batch_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embedding1 = nn.Embedding(voc_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, voc_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embs1 = self.embedding1(torch.tensor(inputs))\n",
    "        z1 = self.linear1(embs1)\n",
    "        log_softmax = F.log_softmax(z1, dim=2)\n",
    "        return log_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training with Exponential Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Epoch 0 ==========\n",
      "Batch 1/17046\n",
      "Loss = 10.885300636291504\n",
      "Learning rate = 0.01 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilbuono/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/17046\n",
      "Loss = 10.476454734802246\n",
      "Learning rate = 0.01 \n",
      "\n",
      "Batch 20/17046\n",
      "Loss = 10.166227340698242\n",
      "Learning rate = 0.009000000000000001 \n",
      "\n",
      "Batch 30/17046\n",
      "Loss = 10.074182510375977\n",
      "Learning rate = 0.008100000000000001 \n",
      "\n",
      "Batch 40/17046\n",
      "Loss = 10.000591278076172\n",
      "Learning rate = 0.007290000000000001 \n",
      "\n",
      "Batch 50/17046\n",
      "Loss = 9.957330703735352\n",
      "Learning rate = 0.006561000000000002 \n",
      "\n",
      "Batch 60/17046\n",
      "Loss = 9.918911933898926\n",
      "Learning rate = 0.005904900000000002 \n",
      "\n",
      "Batch 70/17046\n",
      "Loss = 9.914350509643555\n",
      "Learning rate = 0.005314410000000002 \n",
      "\n",
      "Batch 80/17046\n",
      "Loss = 9.9038724899292\n",
      "Learning rate = 0.004782969000000002 \n",
      "\n",
      "Batch 90/17046\n",
      "Loss = 9.896750450134277\n",
      "Learning rate = 0.004304672100000002 \n",
      "\n",
      "Batch 100/17046\n",
      "Loss = 9.889885902404785\n",
      "Learning rate = 0.003874204890000002 \n",
      "\n",
      "Batch 110/17046\n",
      "Loss = 9.885173797607422\n",
      "Learning rate = 0.003486784401000002 \n",
      "\n",
      "Batch 120/17046\n",
      "Loss = 9.882335662841797\n",
      "Learning rate = 0.003138105960900002 \n",
      "\n",
      "Batch 130/17046\n",
      "Loss = 9.882954597473145\n",
      "Learning rate = 0.0028242953648100018 \n",
      "\n",
      "Batch 140/17046\n",
      "Loss = 9.881354331970215\n",
      "Learning rate = 0.0025418658283290017 \n",
      "\n",
      "Batch 150/17046\n",
      "Loss = 9.879481315612793\n",
      "Learning rate = 0.0022876792454961017 \n",
      "\n",
      "Batch 160/17046\n",
      "Loss = 9.875799179077148\n",
      "Learning rate = 0.0020589113209464917 \n",
      "\n",
      "Batch 170/17046\n",
      "Loss = 9.87606430053711\n",
      "Learning rate = 0.0018530201888518425 \n",
      "\n",
      "Batch 180/17046\n",
      "Loss = 9.877500534057617\n",
      "Learning rate = 0.0016677181699666583 \n",
      "\n",
      "Batch 190/17046\n",
      "Loss = 9.87633991241455\n",
      "Learning rate = 0.0015009463529699924 \n",
      "\n",
      "Batch 200/17046\n",
      "Loss = 9.876274108886719\n",
      "Learning rate = 0.0013508517176729932 \n",
      "\n",
      "Batch 210/17046\n",
      "Loss = 9.876382827758789\n",
      "Learning rate = 0.001215766545905694 \n",
      "\n",
      "Batch 220/17046\n",
      "Loss = 9.875618934631348\n",
      "Learning rate = 0.0010941898913151245 \n",
      "\n",
      "Batch 230/17046\n",
      "Loss = 9.874380111694336\n",
      "Learning rate = 0.0009847709021836122 \n",
      "\n",
      "Batch 240/17046\n",
      "Loss = 9.874563217163086\n",
      "Learning rate = 0.0008862938119652509 \n",
      "\n",
      "Batch 250/17046\n",
      "Loss = 9.874839782714844\n",
      "Learning rate = 0.0007976644307687258 \n",
      "\n",
      "Batch 260/17046\n",
      "Loss = 9.874363899230957\n",
      "Learning rate = 0.0007178979876918532 \n",
      "\n",
      "Batch 270/17046\n",
      "Loss = 9.874738693237305\n",
      "Learning rate = 0.0006461081889226679 \n",
      "\n",
      "Batch 280/17046\n",
      "Loss = 9.874715805053711\n",
      "Learning rate = 0.0005814973700304011 \n",
      "\n",
      "Batch 290/17046\n",
      "Loss = 9.8738374710083\n",
      "Learning rate = 0.0005233476330273611 \n",
      "\n",
      "Batch 300/17046\n",
      "Loss = 9.874296188354492\n",
      "Learning rate = 0.000471012869724625 \n",
      "\n",
      "Batch 310/17046\n",
      "Loss = 9.873374938964844\n",
      "Learning rate = 0.0004239115827521625 \n",
      "\n",
      "Batch 320/17046\n",
      "Loss = 9.87304401397705\n",
      "Learning rate = 0.00038152042447694626 \n",
      "\n",
      "Batch 330/17046\n",
      "Loss = 9.874215126037598\n",
      "Learning rate = 0.00034336838202925164 \n",
      "\n",
      "Batch 340/17046\n",
      "Loss = 9.87386703491211\n",
      "Learning rate = 0.0003090315438263265 \n",
      "\n",
      "Batch 350/17046\n",
      "Loss = 9.874288558959961\n",
      "Learning rate = 0.00027812838944369386 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-37fe3b0c41ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-d804a9129777>\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoc_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(voc_size=batcher.voc_size, embedding_dim=50, window_size=batcher.window_size, batch_size=batcher.batch_size)\n",
    "model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "lr_scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "for epoch in [0, 1, 2]:\n",
    "    print('========== Epoch {} =========='.format(epoch))\n",
    "    total_loss = 0\n",
    "    i = 1\n",
    "    N = int(len(batcher.corpus)//BATCH_SIZE)\n",
    "    for context, target in batcher.generator():\n",
    "        model.train()\n",
    "        context = torch.tensor(context).to(device='cuda')\n",
    "        target = torch.tensor(target).to(device='cuda')\n",
    "        \n",
    "        log_probs = model(context)\n",
    "        loss = loss_function(log_probs, target)\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if i%10==0 or i==1:\n",
    "            print('Batch {}/{}'.format(i, N))\n",
    "            print('Loss = {}'.format(float(loss)))\n",
    "            lr = float(optimizer.param_groups[0]['lr'])\n",
    "            print(\"Learning rate = {}\".format(lr), '\\n')\n",
    "        if i%10==0:\n",
    "            lr_scheduler.step()\n",
    "        i += 1\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training with ReduceLROnPlateau Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Epoch 0 ==========\n",
      "Batch 1/170167\n",
      "Loss = 12.149\n",
      "Learning rate = 0.01 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilbuono/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/170167\n",
      "Loss = 11.177\n",
      "Learning rate = 0.01 \n",
      "\n",
      "Batch 200/170167\n",
      "Loss = 11.173\n",
      "Learning rate = 0.01 \n",
      "\n",
      "Batch 300/170167\n",
      "Loss = 11.172\n",
      "Learning rate = 0.01 \n",
      "\n",
      "Batch 400/170167\n",
      "Loss = 11.172\n",
      "Learning rate = 0.01 \n",
      "\n",
      "Batch 500/170167\n",
      "Loss = 11.172\n",
      "Learning rate = 0.01 \n",
      "\n",
      "Batch 600/170167\n",
      "Loss = 11.172\n",
      "Learning rate = 0.01 \n",
      "\n",
      "Batch 700/170167\n",
      "Loss = 11.172\n",
      "Learning rate = 0.01 \n",
      "\n",
      "Batch 800/170167\n",
      "Loss = 11.172\n",
      "Learning rate = 0.01 \n",
      "\n",
      "Batch 900/170167\n",
      "Loss = 11.173\n",
      "Learning rate = 0.01 \n",
      "\n",
      "Batch 1000/170167\n",
      "Loss = 11.173\n",
      "Learning rate = 0.01 \n",
      "\n",
      "Batch 1100/170167\n",
      "Loss = 11.174\n",
      "Learning rate = 0.01 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(voc_size=batcher.voc_size, embedding_dim=256, window_size=batcher.window_size, batch_size=batcher.batch_size)\n",
    "model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer = optimizer, \\\n",
    "                                 mode = 'min', \\\n",
    "                                 factor = 0.5, \\\n",
    "                                 threshold = 0.001 \\\n",
    "                                )\n",
    "\n",
    "for epoch in [0, 1, 2]:\n",
    "    print('========== Epoch {} =========='.format(epoch))\n",
    "    total_loss = 0\n",
    "    i = 1\n",
    "    N = int(len(batcher.corpus)//BATCH_SIZE)\n",
    "    for context, target in batcher.generator():\n",
    "        model.train()\n",
    "        context = torch.tensor(context).to(device='cuda')\n",
    "        target = torch.tensor(target).to(device='cuda')\n",
    "        \n",
    "        log_probs = model(context)\n",
    "        loss = loss_function(log_probs, target)\n",
    "        \n",
    "        if i%100==0 or i==1:\n",
    "            print('Batch {}/{}'.format(i, N))\n",
    "            print('Loss = {}'.format(round(float(loss), 3)))\n",
    "            lr = float(optimizer.param_groups[0]['lr'])\n",
    "            print(\"Learning rate = {}\".format(lr), '\\n')\n",
    "        if i%100==0:\n",
    "            lr_scheduler.step(loss)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        i += 1\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

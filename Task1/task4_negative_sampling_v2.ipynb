{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1.4: Negative sampling (15 points)\n",
    "\n",
    "You may have noticed that word2vec is really slow to train. Especially with big (> 50 000) vocabularies. Negative sampling is the solution.\n",
    "\n",
    "The task is to implement word2vec with negative sampling.\n",
    "\n",
    "This is what was discussed in Stanford lecture. The main idea is in the formula:\n",
    "\n",
    "$$ L = \\log\\sigma(u^T_o \\cdot u_c) + \\sum^k_{i=1} \\mathbb{E}_{j \\sim P(w)}[\\log\\sigma(-u^T_j \\cdot u_c)]$$\n",
    "\n",
    "Where $\\sigma$ - sigmoid function, $u_c$ - central word vector, $u_o$ - context (outside of the window) word vector, $u_j$ - vector or word with index $j$.\n",
    "\n",
    "The first term calculates the similarity between positive examples (word from one window)\n",
    "\n",
    "The second term is responsible for negative samples. $k$ is a hyperparameter - the number of negatives to sample.\n",
    "$\\mathbb{E}_{j \\sim P(w)}$\n",
    "means that $j$ is distributed accordingly to unigram distribution.\n",
    "\n",
    "Thus, it is only required to calculate the similarity between positive samples and some other negatives. Not across all the vocabulary.\n",
    "\n",
    "Useful links:\n",
    "1. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "1. [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "gc.collect()\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "len(STOP_WORDS)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import seaborn as sns\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print (torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create own batcher with batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batcher:\n",
    "    def __init__(self, max_len, window_size, corpus_path, min_freq, max_freq, max_voc_size, batch_size):\n",
    "        self.corpus_path = corpus_path\n",
    "        self.window_size = window_size\n",
    "        self.min_freq = min_freq\n",
    "        self.max_freq = max_freq\n",
    "        self.max_voc_size = max_voc_size\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "        self.words = None\n",
    "        self.word2index = None\n",
    "        self.index2word = None\n",
    "        self.freq = None\n",
    "        self.voc = None\n",
    "        self.voc_size = None\n",
    "        self.corpus = None\n",
    "        self.corpus_size = None  \n",
    "        \n",
    "    def read_data(self, S):\n",
    "        if S == None:\n",
    "            with open(self.corpus_path, 'r') as f:\n",
    "                S = f.read()\n",
    "            if S!=None:\n",
    "                S = S.lower()[: self.max_len]\n",
    "        print('Len of S = ', len(S))\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        S = regex.sub(' ', S)\n",
    "        words_raw = list(S.split())\n",
    "        print(len(words_raw))\n",
    "        words = []\n",
    "        for word in words_raw:\n",
    "            if word in STOP_WORDS:\n",
    "                pass\n",
    "            else:\n",
    "                words.append(word)\n",
    "\n",
    "        print('Size of words = ', len(words))\n",
    "        counter = Counter(words)\n",
    "        print('Size of counter = ', len(counter))\n",
    "        if self.min_freq != None:\n",
    "            counter = {x : counter[x] for x in counter if counter[x] >= self.min_freq}\n",
    "        print('Size of counter after min_freq = ', len(counter))\n",
    "        if self.max_freq != None:\n",
    "            counter = {x : counter[x] for x in counter if counter[x] <= self.max_freq}\n",
    "        print('Size of counter after max_freq = ', len(counter))\n",
    "        counter = Counter(counter)\n",
    "\n",
    "        freq = dict(counter.most_common(self.max_voc_size))\n",
    "        voc = set(freq)\n",
    "        \n",
    "        unk = set(words).difference(voc)\n",
    "        print('Size of freq dict = ', len(voc))\n",
    "        print('Number of vocabulary words = ', len(voc))\n",
    "        print('Number of unknown words = ', len(unk))\n",
    "\n",
    "        words = ['UNK' if word in unk else word for word in words]        \n",
    "        if len(words)%self.batch_size == 0:\n",
    "            padding = self.window_size\n",
    "        else:\n",
    "            padding = self.batch_size - len(words)%self.batch_size + self.window_size\n",
    "            \n",
    "        words = ['PAD']*self.window_size + words + ['PAD']*padding\n",
    "        unique_words = list(set(words))\n",
    "        print('Size of corpus = ', len(words))\n",
    "        print('Size of vocabulary = ', len(unique_words))\n",
    "        self.word2index = {k: v for v, k in enumerate(unique_words)}\n",
    "        self.index2word = {v: k for v, k in enumerate(unique_words)}\n",
    "        words = [self.word2index[word] for word in words]\n",
    "        self.freq = Counter(words)\n",
    "        self.voc = set(self.freq)\n",
    "        self.voc_size = len(self.voc)\n",
    "        self.corpus = words\n",
    "        self.corpus_size = len(words)\n",
    "    \n",
    "    def generator(self):\n",
    "        i = self.window_size\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        while i < self.corpus_size-self.window_size:\n",
    "            if len(x_batch)==self.batch_size:\n",
    "                x_batch = []\n",
    "                y_batch = []\n",
    "                \n",
    "            x = self.corpus[i-self.window_size: i] + self.corpus[i+1: i+self.window_size+1]\n",
    "            y = [self.corpus[i]]*self.window_size*2\n",
    "            x_batch.append(x)\n",
    "            y_batch.append(y)\n",
    "            i += 1\n",
    "            if len(x_batch)==self.batch_size:\n",
    "                yield np.array(x_batch), np.array(y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Batcher with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of S =  100000000\n",
      "17005207\n",
      "Size of words =  10890638\n",
      "Size of counter =  253702\n",
      "Size of counter after min_freq =  71140\n",
      "Size of counter after max_freq =  71140\n",
      "Size of freq dict =  71140\n",
      "Number of vocabulary words =  71140\n",
      "Number of unknown words =  182562\n",
      "Size of corpus =  10895364\n",
      "Size of vocabulary =  71142\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8192\n",
    "MAX_LEN = 100000000000\n",
    "batcher = Batcher(max_len=MAX_LEN, window_size=2, corpus_path='text8', min_freq=5, max_freq=None, max_voc_size=10000000, batch_size=BATCH_SIZE)\n",
    "batcher.read_data(S=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check dimentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8192, 4) (8192, 4)\n"
     ]
    }
   ],
   "source": [
    "for x, y in batcher.generator():\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check value of the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x, y in batcher.generator():\n",
    "    for i in range(x.shape[0]):\n",
    "        target_word = y[i][0]\n",
    "        for j in range(x.shape[1]):\n",
    "            context_word = x[i][j]\n",
    "            print(batcher.index2word[target_word], batcher.index2word[context_word])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CBOW with naive Negative Sampling class using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, voc_size, embedding_dim, window_size, batch_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embedding1 = nn.Embedding(voc_size, embedding_dim)\n",
    "        self.embedding2 = nn.Embedding(voc_size, embedding_dim)\n",
    "        \n",
    "    def forward(self, target_word, context_word):\n",
    "        target_word = torch.tensor(target_word).to(device='cuda')\n",
    "        context_word = torch.tensor(context_word).to(device='cuda')\n",
    "        target_emb = self.embedding1(target_word)\n",
    "        context_emb = self.embedding2(context_word)\n",
    "        z1 = torch.mul(target_emb, context_emb)\n",
    "        z2 = torch.sum(z1, dim=2)\n",
    "        pos_loss = torch.sum(F.logsigmoid(z2))\n",
    "\n",
    "        neg_loss = 0\n",
    "        for i in range(5):\n",
    "            negative_word = choice(batcher.corpus, size=batcher.batch_size*batcher.window_size*2).reshape((batcher.batch_size, batcher.window_size*2))\n",
    "            negative_word = torch.tensor(numpy.random.choice(batcher.corpus)).to(device='cuda')\n",
    "            negative_emb = self.embedding2(negative_word)\n",
    "            z4 = torch.mul(target_emb, negative_emb)\n",
    "            z5 = torch.sum(z4, dim=2)\n",
    "            z6 = torch.sum(F.logsigmoid(-z5))\n",
    "            neg_loss += z6\n",
    "        \n",
    "        return -(pos_loss + neg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training with Negative Sampling and Exponential Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Epoch 0 ==========\n",
      "Batch 0/134 loss : 42.06694030761719 \n",
      "\n",
      "Batch 1/134 loss : 41.704933166503906 \n",
      "\n",
      "Batch 2/134 loss : 39.430267333984375 \n",
      "\n",
      "Batch 3/134 loss : 38.71807861328125 \n",
      "\n",
      "Batch 4/134 loss : 39.73075866699219 \n",
      "\n",
      "Batch 5/134 loss : 40.66918182373047 \n",
      "\n",
      "Batch 6/134 loss : 38.6294059753418 \n",
      "\n",
      "Batch 7/134 loss : 40.35169982910156 \n",
      "\n",
      "Batch 8/134 loss : 38.260215759277344 \n",
      "\n",
      "Batch 9/134 loss : 39.16942596435547 \n",
      "\n",
      "Batch 10/134 loss : 38.10734558105469 \n",
      "\n",
      "Batch 11/134 loss : 35.58789825439453 \n",
      "\n",
      "Batch 12/134 loss : 34.93800354003906 \n",
      "\n",
      "Batch 13/134 loss : 35.9769172668457 \n",
      "\n",
      "Batch 14/134 loss : 35.67118453979492 \n",
      "\n",
      "Batch 15/134 loss : 35.47370529174805 \n",
      "\n",
      "Batch 16/134 loss : 37.17887878417969 \n",
      "\n",
      "Batch 17/134 loss : 35.50787353515625 \n",
      "\n",
      "Batch 18/134 loss : 35.08230972290039 \n",
      "\n",
      "Batch 19/134 loss : 33.56196212768555 \n",
      "\n",
      "Batch 20/134 loss : 38.42351150512695 \n",
      "\n",
      "Batch 21/134 loss : 37.90135955810547 \n",
      "\n",
      "Batch 22/134 loss : 41.83424377441406 \n",
      "\n",
      "Batch 23/134 loss : 39.043792724609375 \n",
      "\n",
      "Batch 24/134 loss : 34.973541259765625 \n",
      "\n",
      "Batch 25/134 loss : 32.5562858581543 \n",
      "\n",
      "Batch 26/134 loss : 36.351959228515625 \n",
      "\n",
      "Batch 27/134 loss : 30.38631820678711 \n",
      "\n",
      "Batch 28/134 loss : 37.383522033691406 \n",
      "\n",
      "Batch 29/134 loss : 37.166255950927734 \n",
      "\n",
      "Batch 30/134 loss : 32.486000061035156 \n",
      "\n",
      "Batch 31/134 loss : 39.58357238769531 \n",
      "\n",
      "Batch 32/134 loss : 36.45850372314453 \n",
      "\n",
      "Batch 33/134 loss : 31.845718383789062 \n",
      "\n",
      "Batch 34/134 loss : 30.430946350097656 \n",
      "\n",
      "Batch 35/134 loss : 34.30722427368164 \n",
      "\n",
      "Batch 36/134 loss : 30.554489135742188 \n",
      "\n",
      "Batch 37/134 loss : 34.78593826293945 \n",
      "\n",
      "Batch 38/134 loss : 32.03260803222656 \n",
      "\n",
      "Batch 39/134 loss : 34.1773796081543 \n",
      "\n",
      "Batch 40/134 loss : 28.47765350341797 \n",
      "\n",
      "Batch 41/134 loss : 28.82284164428711 \n",
      "\n",
      "Batch 42/134 loss : 28.535106658935547 \n",
      "\n",
      "Batch 43/134 loss : 33.39093780517578 \n",
      "\n",
      "Batch 44/134 loss : 31.4946231842041 \n",
      "\n",
      "Batch 45/134 loss : 37.601993560791016 \n",
      "\n",
      "Batch 46/134 loss : 30.660884857177734 \n",
      "\n",
      "Batch 47/134 loss : 34.19495391845703 \n",
      "\n",
      "Batch 48/134 loss : 35.285335540771484 \n",
      "\n",
      "Batch 49/134 loss : 39.222503662109375 \n",
      "\n",
      "Batch 50/134 loss : 34.008262634277344 \n",
      "\n",
      "Batch 51/134 loss : 28.047969818115234 \n",
      "\n",
      "Batch 52/134 loss : 34.25275421142578 \n",
      "\n",
      "Batch 53/134 loss : 33.31488800048828 \n",
      "\n",
      "Batch 54/134 loss : 30.31597137451172 \n",
      "\n",
      "Batch 55/134 loss : 32.043067932128906 \n",
      "\n",
      "Batch 56/134 loss : 29.091379165649414 \n",
      "\n",
      "Batch 57/134 loss : 35.156944274902344 \n",
      "\n",
      "Batch 58/134 loss : 30.518016815185547 \n",
      "\n",
      "Batch 59/134 loss : 39.115882873535156 \n",
      "\n",
      "Batch 60/134 loss : 36.67921447753906 \n",
      "\n",
      "Batch 61/134 loss : 23.165584564208984 \n",
      "\n",
      "Batch 62/134 loss : 31.87206268310547 \n",
      "\n",
      "Batch 63/134 loss : 33.33769226074219 \n",
      "\n",
      "Batch 64/134 loss : 29.632524490356445 \n",
      "\n",
      "Batch 65/134 loss : 29.289134979248047 \n",
      "\n",
      "Batch 66/134 loss : 30.52364158630371 \n",
      "\n",
      "Batch 67/134 loss : 29.29818344116211 \n",
      "\n",
      "Batch 68/134 loss : 20.69849395751953 \n",
      "\n",
      "Batch 69/134 loss : 32.57405090332031 \n",
      "\n",
      "Batch 70/134 loss : 25.373228073120117 \n",
      "\n",
      "Batch 71/134 loss : 32.0664176940918 \n",
      "\n",
      "Batch 72/134 loss : 25.24863624572754 \n",
      "\n",
      "Batch 73/134 loss : 32.2928581237793 \n",
      "\n",
      "Batch 74/134 loss : 26.556854248046875 \n",
      "\n",
      "Batch 75/134 loss : 29.444122314453125 \n",
      "\n",
      "Batch 76/134 loss : 25.54145050048828 \n",
      "\n",
      "Batch 77/134 loss : 31.537063598632812 \n",
      "\n",
      "Batch 78/134 loss : 31.545696258544922 \n",
      "\n",
      "Batch 79/134 loss : 31.51108169555664 \n",
      "\n",
      "Batch 80/134 loss : 31.582794189453125 \n",
      "\n",
      "Batch 81/134 loss : 28.75344467163086 \n",
      "\n",
      "Batch 82/134 loss : 30.495956420898438 \n",
      "\n",
      "Batch 83/134 loss : 31.224998474121094 \n",
      "\n",
      "Batch 84/134 loss : 28.597938537597656 \n",
      "\n",
      "Batch 85/134 loss : 27.675769805908203 \n",
      "\n",
      "Batch 86/134 loss : 25.83419418334961 \n",
      "\n",
      "Batch 87/134 loss : 29.873004913330078 \n",
      "\n",
      "Batch 88/134 loss : 21.646156311035156 \n",
      "\n",
      "Batch 89/134 loss : 33.43819808959961 \n",
      "\n",
      "Batch 90/134 loss : 31.882829666137695 \n",
      "\n",
      "Batch 91/134 loss : 31.265869140625 \n",
      "\n",
      "Batch 92/134 loss : 29.064373016357422 \n",
      "\n",
      "Batch 93/134 loss : 23.812347412109375 \n",
      "\n",
      "Batch 94/134 loss : 29.359752655029297 \n",
      "\n",
      "Batch 95/134 loss : 27.516735076904297 \n",
      "\n",
      "Batch 96/134 loss : 26.997394561767578 \n",
      "\n",
      "Batch 97/134 loss : 22.404571533203125 \n",
      "\n",
      "Batch 98/134 loss : 34.97916030883789 \n",
      "\n",
      "Batch 99/134 loss : 23.155792236328125 \n",
      "\n",
      "Batch 100/134 loss : 34.091552734375 \n",
      "\n",
      "Batch 101/134 loss : 33.79403305053711 \n",
      "\n",
      "Batch 102/134 loss : 31.55711555480957 \n",
      "\n",
      "Batch 103/134 loss : 26.969694137573242 \n",
      "\n",
      "Batch 104/134 loss : 30.3898868560791 \n",
      "\n",
      "Batch 105/134 loss : 22.209383010864258 \n",
      "\n",
      "Batch 106/134 loss : 30.573101043701172 \n",
      "\n",
      "Batch 107/134 loss : 29.780353546142578 \n",
      "\n",
      "Batch 108/134 loss : 35.02907943725586 \n",
      "\n",
      "Batch 109/134 loss : 34.3913459777832 \n",
      "\n",
      "Batch 110/134 loss : 29.761770248413086 \n",
      "\n",
      "Batch 111/134 loss : 28.418933868408203 \n",
      "\n",
      "Batch 112/134 loss : 31.50244903564453 \n",
      "\n",
      "Batch 113/134 loss : 30.8531494140625 \n",
      "\n",
      "Batch 114/134 loss : 26.82628059387207 \n",
      "\n",
      "Batch 115/134 loss : 28.265724182128906 \n",
      "\n",
      "Batch 116/134 loss : 20.601879119873047 \n",
      "\n",
      "Batch 117/134 loss : 27.417068481445312 \n",
      "\n",
      "Batch 118/134 loss : 25.424659729003906 \n",
      "\n",
      "Batch 119/134 loss : 32.00178909301758 \n",
      "\n",
      "Batch 120/134 loss : 25.842134475708008 \n",
      "\n",
      "Batch 121/134 loss : 25.070432662963867 \n",
      "\n",
      "Batch 122/134 loss : 37.49971389770508 \n",
      "\n",
      "Batch 123/134 loss : 27.60669708251953 \n",
      "\n",
      "Batch 124/134 loss : 29.03297233581543 \n",
      "\n",
      "Batch 125/134 loss : 20.73552894592285 \n",
      "\n",
      "Batch 126/134 loss : 26.47282600402832 \n",
      "\n",
      "Batch 127/134 loss : 30.53915023803711 \n",
      "\n",
      "Batch 128/134 loss : 19.01430320739746 \n",
      "\n",
      "Batch 129/134 loss : 18.23702049255371 \n",
      "\n",
      "Batch 130/134 loss : 28.167383193969727 \n",
      "\n",
      "Batch 131/134 loss : 28.81301498413086 \n",
      "\n",
      "Batch 132/134 loss : 23.682518005371094 \n",
      "\n",
      "Batch 133/134 loss : 52.62561798095703 \n",
      "\n",
      "========== Epoch 1 ==========\n",
      "Batch 0/134 loss : 25.29741668701172 \n",
      "\n",
      "Batch 1/134 loss : 32.079586029052734 \n",
      "\n",
      "Batch 2/134 loss : 28.242687225341797 \n",
      "\n",
      "Batch 3/134 loss : 29.42474365234375 \n",
      "\n",
      "Batch 4/134 loss : 23.78742027282715 \n",
      "\n",
      "Batch 5/134 loss : 32.133888244628906 \n",
      "\n",
      "Batch 6/134 loss : 25.765169143676758 \n",
      "\n",
      "Batch 7/134 loss : 26.777910232543945 \n",
      "\n",
      "Batch 8/134 loss : 22.322467803955078 \n",
      "\n",
      "Batch 9/134 loss : 26.86990737915039 \n",
      "\n",
      "Batch 10/134 loss : 27.882028579711914 \n",
      "\n",
      "Batch 11/134 loss : 28.222423553466797 \n",
      "\n",
      "Batch 12/134 loss : 21.96044921875 \n",
      "\n",
      "Batch 13/134 loss : 29.3037166595459 \n",
      "\n",
      "Batch 14/134 loss : 36.208675384521484 \n",
      "\n",
      "Batch 15/134 loss : 28.335573196411133 \n",
      "\n",
      "Batch 16/134 loss : 20.102563858032227 \n",
      "\n",
      "Batch 17/134 loss : 35.82475280761719 \n",
      "\n",
      "Batch 18/134 loss : 31.866228103637695 \n",
      "\n",
      "Batch 19/134 loss : 27.541433334350586 \n",
      "\n",
      "Batch 20/134 loss : 31.190319061279297 \n",
      "\n",
      "Batch 21/134 loss : 21.3917236328125 \n",
      "\n",
      "Batch 22/134 loss : 27.0529842376709 \n",
      "\n",
      "Batch 23/134 loss : 24.34264373779297 \n",
      "\n",
      "Batch 24/134 loss : 31.425424575805664 \n",
      "\n",
      "Batch 25/134 loss : 27.636764526367188 \n",
      "\n",
      "Batch 26/134 loss : 30.648767471313477 \n",
      "\n",
      "Batch 27/134 loss : 19.113096237182617 \n",
      "\n",
      "Batch 28/134 loss : 27.357067108154297 \n",
      "\n",
      "Batch 29/134 loss : 20.072418212890625 \n",
      "\n",
      "Batch 30/134 loss : 30.696191787719727 \n",
      "\n",
      "Batch 31/134 loss : 26.107742309570312 \n",
      "\n",
      "Batch 32/134 loss : 26.983476638793945 \n",
      "\n",
      "Batch 33/134 loss : 29.657447814941406 \n",
      "\n",
      "Batch 34/134 loss : 25.139049530029297 \n",
      "\n",
      "Batch 35/134 loss : 19.053422927856445 \n",
      "\n",
      "Batch 36/134 loss : 29.979293823242188 \n",
      "\n",
      "Batch 37/134 loss : 26.937728881835938 \n",
      "\n",
      "Batch 38/134 loss : 29.328702926635742 \n",
      "\n",
      "Batch 39/134 loss : 17.60982894897461 \n",
      "\n",
      "Batch 40/134 loss : 30.598745346069336 \n",
      "\n",
      "Batch 41/134 loss : 21.47410774230957 \n",
      "\n",
      "Batch 42/134 loss : 24.044269561767578 \n",
      "\n",
      "Batch 43/134 loss : 29.909870147705078 \n",
      "\n",
      "Batch 44/134 loss : 27.731266021728516 \n",
      "\n",
      "Batch 45/134 loss : 30.0211124420166 \n",
      "\n",
      "Batch 46/134 loss : 25.793027877807617 \n",
      "\n",
      "Batch 47/134 loss : 30.543621063232422 \n",
      "\n",
      "Batch 48/134 loss : 27.250083923339844 \n",
      "\n",
      "Batch 49/134 loss : 22.642786026000977 \n",
      "\n",
      "Batch 50/134 loss : 29.89137077331543 \n",
      "\n",
      "Batch 51/134 loss : 25.971845626831055 \n",
      "\n",
      "Batch 52/134 loss : 26.177974700927734 \n",
      "\n",
      "Batch 53/134 loss : 24.372846603393555 \n",
      "\n",
      "Batch 54/134 loss : 30.080360412597656 \n",
      "\n",
      "Batch 55/134 loss : 26.557138442993164 \n",
      "\n",
      "Batch 56/134 loss : 20.869482040405273 \n",
      "\n",
      "Batch 57/134 loss : 22.961444854736328 \n",
      "\n",
      "Batch 58/134 loss : 27.359283447265625 \n",
      "\n",
      "Batch 59/134 loss : 29.868213653564453 \n",
      "\n",
      "Batch 60/134 loss : 30.815671920776367 \n",
      "\n",
      "Batch 61/134 loss : 25.810598373413086 \n",
      "\n",
      "Batch 62/134 loss : 22.89826011657715 \n",
      "\n",
      "Batch 63/134 loss : 20.0399169921875 \n",
      "\n",
      "Batch 64/134 loss : 22.979469299316406 \n",
      "\n",
      "Batch 65/134 loss : 30.149333953857422 \n",
      "\n",
      "Batch 66/134 loss : 22.101341247558594 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 67/134 loss : 27.82274627685547 \n",
      "\n",
      "Batch 68/134 loss : 24.64341163635254 \n",
      "\n",
      "Batch 69/134 loss : 15.150001525878906 \n",
      "\n",
      "Batch 70/134 loss : 24.52313232421875 \n",
      "\n",
      "Batch 71/134 loss : 15.214583396911621 \n",
      "\n",
      "Batch 72/134 loss : 16.40041732788086 \n",
      "\n",
      "Batch 73/134 loss : 21.042037963867188 \n",
      "\n",
      "Batch 74/134 loss : 23.99235725402832 \n",
      "\n",
      "Batch 75/134 loss : 22.24390411376953 \n",
      "\n",
      "Batch 76/134 loss : 29.38700294494629 \n",
      "\n",
      "Batch 77/134 loss : 30.703094482421875 \n",
      "\n",
      "Batch 78/134 loss : 25.650856018066406 \n",
      "\n",
      "Batch 79/134 loss : 27.229095458984375 \n",
      "\n",
      "Batch 80/134 loss : 25.245319366455078 \n",
      "\n",
      "Batch 81/134 loss : 29.829444885253906 \n",
      "\n",
      "Batch 82/134 loss : 29.383737564086914 \n",
      "\n",
      "Batch 83/134 loss : 25.412403106689453 \n",
      "\n",
      "Batch 84/134 loss : 13.308046340942383 \n",
      "\n",
      "Batch 85/134 loss : 21.357872009277344 \n",
      "\n",
      "Batch 86/134 loss : 19.464984893798828 \n",
      "\n",
      "Batch 87/134 loss : 23.710956573486328 \n",
      "\n",
      "Batch 88/134 loss : 28.732437133789062 \n",
      "\n",
      "Batch 89/134 loss : 15.417354583740234 \n",
      "\n",
      "Batch 90/134 loss : 20.17707061767578 \n",
      "\n",
      "Batch 91/134 loss : 16.985754013061523 \n",
      "\n",
      "Batch 92/134 loss : 21.343034744262695 \n",
      "\n",
      "Batch 93/134 loss : 24.510181427001953 \n",
      "\n",
      "Batch 94/134 loss : 25.663991928100586 \n",
      "\n",
      "Batch 95/134 loss : 31.69926643371582 \n",
      "\n",
      "Batch 96/134 loss : 32.64281463623047 \n",
      "\n",
      "Batch 97/134 loss : 19.401721954345703 \n",
      "\n",
      "Batch 98/134 loss : 24.264785766601562 \n",
      "\n",
      "Batch 99/134 loss : 31.093168258666992 \n",
      "\n",
      "Batch 100/134 loss : 23.34874725341797 \n",
      "\n",
      "Batch 101/134 loss : 30.552499771118164 \n",
      "\n",
      "Batch 102/134 loss : 29.40671157836914 \n",
      "\n",
      "Batch 103/134 loss : 19.02254867553711 \n",
      "\n",
      "Batch 104/134 loss : 31.85783576965332 \n",
      "\n",
      "Batch 105/134 loss : 22.745925903320312 \n",
      "\n",
      "Batch 106/134 loss : 26.615419387817383 \n",
      "\n",
      "Batch 107/134 loss : 20.913930892944336 \n",
      "\n",
      "Batch 108/134 loss : 25.059898376464844 \n",
      "\n",
      "Batch 109/134 loss : 29.791481018066406 \n",
      "\n",
      "Batch 110/134 loss : 23.623493194580078 \n",
      "\n",
      "Batch 111/134 loss : 31.076921463012695 \n",
      "\n",
      "Batch 112/134 loss : 17.37258529663086 \n",
      "\n",
      "Batch 113/134 loss : 15.921408653259277 \n",
      "\n",
      "Batch 114/134 loss : 26.024263381958008 \n",
      "\n",
      "Batch 115/134 loss : 22.693071365356445 \n",
      "\n",
      "Batch 116/134 loss : 24.529396057128906 \n",
      "\n",
      "Batch 117/134 loss : 20.650192260742188 \n",
      "\n",
      "Batch 118/134 loss : 18.04671859741211 \n",
      "\n",
      "Batch 119/134 loss : 20.860742568969727 \n",
      "\n",
      "Batch 120/134 loss : 21.372800827026367 \n",
      "\n",
      "Batch 121/134 loss : 27.72416114807129 \n",
      "\n",
      "Batch 122/134 loss : 25.546907424926758 \n",
      "\n",
      "Batch 123/134 loss : 21.130634307861328 \n",
      "\n",
      "Batch 124/134 loss : 33.05577850341797 \n",
      "\n",
      "Batch 125/134 loss : 14.622501373291016 \n",
      "\n",
      "Batch 126/134 loss : 26.716936111450195 \n",
      "\n",
      "Batch 127/134 loss : 22.00759506225586 \n",
      "\n",
      "Batch 128/134 loss : 25.254241943359375 \n",
      "\n",
      "Batch 129/134 loss : 15.744108200073242 \n",
      "\n",
      "Batch 130/134 loss : 23.725618362426758 \n",
      "\n",
      "Batch 131/134 loss : 20.60028076171875 \n",
      "\n",
      "Batch 132/134 loss : 20.694190979003906 \n",
      "\n",
      "Batch 133/134 loss : 24.26264762878418 \n",
      "\n",
      "========== Epoch 2 ==========\n",
      "Batch 0/134 loss : 20.438274383544922 \n",
      "\n",
      "Batch 1/134 loss : 24.284780502319336 \n",
      "\n",
      "Batch 2/134 loss : 22.5312442779541 \n",
      "\n",
      "Batch 3/134 loss : 19.512531280517578 \n",
      "\n",
      "Batch 4/134 loss : 22.28767967224121 \n",
      "\n",
      "Batch 5/134 loss : 25.27903938293457 \n",
      "\n",
      "Batch 6/134 loss : 24.740964889526367 \n",
      "\n",
      "Batch 7/134 loss : 27.654117584228516 \n",
      "\n",
      "Batch 8/134 loss : 27.753829956054688 \n",
      "\n",
      "Batch 9/134 loss : 19.638538360595703 \n",
      "\n",
      "Batch 10/134 loss : 27.88838768005371 \n",
      "\n",
      "Batch 11/134 loss : 30.334928512573242 \n",
      "\n",
      "Batch 12/134 loss : 17.101835250854492 \n",
      "\n",
      "Batch 13/134 loss : 24.765846252441406 \n",
      "\n",
      "Batch 14/134 loss : 25.120922088623047 \n",
      "\n",
      "Batch 15/134 loss : 20.166622161865234 \n",
      "\n",
      "Batch 16/134 loss : 23.019861221313477 \n",
      "\n",
      "Batch 17/134 loss : 27.411365509033203 \n",
      "\n",
      "Batch 18/134 loss : 14.824249267578125 \n",
      "\n",
      "Batch 19/134 loss : 23.687868118286133 \n",
      "\n",
      "Batch 20/134 loss : 24.01256561279297 \n",
      "\n",
      "Batch 21/134 loss : 24.576034545898438 \n",
      "\n",
      "Batch 22/134 loss : 12.975542068481445 \n",
      "\n",
      "Batch 23/134 loss : 22.742725372314453 \n",
      "\n",
      "Batch 24/134 loss : 19.12466049194336 \n",
      "\n",
      "Batch 25/134 loss : 22.593101501464844 \n",
      "\n",
      "Batch 26/134 loss : 26.960216522216797 \n",
      "\n",
      "Batch 27/134 loss : 32.596107482910156 \n",
      "\n",
      "Batch 28/134 loss : 22.90835189819336 \n",
      "\n",
      "Batch 29/134 loss : 19.750646591186523 \n",
      "\n",
      "Batch 30/134 loss : 23.71722984313965 \n",
      "\n",
      "Batch 31/134 loss : 22.356287002563477 \n",
      "\n",
      "Batch 32/134 loss : 20.482402801513672 \n",
      "\n",
      "Batch 33/134 loss : 23.257078170776367 \n",
      "\n",
      "Batch 34/134 loss : 26.365928649902344 \n",
      "\n",
      "Batch 35/134 loss : 13.818903923034668 \n",
      "\n",
      "Batch 36/134 loss : 26.96038246154785 \n",
      "\n",
      "Batch 37/134 loss : 22.09955406188965 \n",
      "\n",
      "Batch 38/134 loss : 25.103864669799805 \n",
      "\n",
      "Batch 39/134 loss : 25.681884765625 \n",
      "\n",
      "Batch 40/134 loss : 17.78416633605957 \n",
      "\n",
      "Batch 41/134 loss : 15.641048431396484 \n",
      "\n",
      "Batch 42/134 loss : 17.042030334472656 \n",
      "\n",
      "Batch 43/134 loss : 26.021163940429688 \n",
      "\n",
      "Batch 44/134 loss : 25.55352210998535 \n",
      "\n",
      "Batch 45/134 loss : 21.888408660888672 \n",
      "\n",
      "Batch 46/134 loss : 24.08565330505371 \n",
      "\n",
      "Batch 47/134 loss : 16.764366149902344 \n",
      "\n",
      "Batch 48/134 loss : 25.43973731994629 \n",
      "\n",
      "Batch 49/134 loss : 22.65848159790039 \n",
      "\n",
      "Batch 50/134 loss : 25.041236877441406 \n",
      "\n",
      "Batch 51/134 loss : 20.06732177734375 \n",
      "\n",
      "Batch 52/134 loss : 24.64544677734375 \n",
      "\n",
      "Batch 53/134 loss : 27.63610076904297 \n",
      "\n",
      "Batch 54/134 loss : 24.85929298400879 \n",
      "\n",
      "Batch 55/134 loss : 24.286481857299805 \n",
      "\n",
      "Batch 56/134 loss : 23.933284759521484 \n",
      "\n",
      "Batch 57/134 loss : 20.812578201293945 \n",
      "\n",
      "Batch 58/134 loss : 22.555429458618164 \n",
      "\n",
      "Batch 59/134 loss : 18.26168441772461 \n",
      "\n",
      "Batch 60/134 loss : 23.70462989807129 \n",
      "\n",
      "Batch 61/134 loss : 21.96938133239746 \n",
      "\n",
      "Batch 62/134 loss : 23.829971313476562 \n",
      "\n",
      "Batch 63/134 loss : 22.927490234375 \n",
      "\n",
      "Batch 64/134 loss : 26.831308364868164 \n",
      "\n",
      "Batch 65/134 loss : 17.745779037475586 \n",
      "\n",
      "Batch 66/134 loss : 21.577518463134766 \n",
      "\n",
      "Batch 67/134 loss : 18.226301193237305 \n",
      "\n",
      "Batch 68/134 loss : 22.137033462524414 \n",
      "\n",
      "Batch 69/134 loss : 16.324495315551758 \n",
      "\n",
      "Batch 70/134 loss : 20.900054931640625 \n",
      "\n",
      "Batch 71/134 loss : 17.47629737854004 \n",
      "\n",
      "Batch 72/134 loss : 11.315464973449707 \n",
      "\n",
      "Batch 73/134 loss : 15.405251502990723 \n",
      "\n",
      "Batch 74/134 loss : 17.147260665893555 \n",
      "\n",
      "Batch 75/134 loss : 24.973072052001953 \n",
      "\n",
      "Batch 76/134 loss : 23.411235809326172 \n",
      "\n",
      "Batch 77/134 loss : 20.35529136657715 \n",
      "\n",
      "Batch 78/134 loss : 28.042491912841797 \n",
      "\n",
      "Batch 79/134 loss : 18.74406623840332 \n",
      "\n",
      "Batch 80/134 loss : 18.409582138061523 \n",
      "\n",
      "Batch 81/134 loss : 28.441082000732422 \n",
      "\n",
      "Batch 82/134 loss : 22.0858097076416 \n",
      "\n",
      "Batch 83/134 loss : 26.696468353271484 \n",
      "\n",
      "Batch 84/134 loss : 18.582162857055664 \n",
      "\n",
      "Batch 85/134 loss : 14.378498077392578 \n",
      "\n",
      "Batch 86/134 loss : 25.612606048583984 \n",
      "\n",
      "Batch 87/134 loss : 24.322307586669922 \n",
      "\n",
      "Batch 88/134 loss : 23.98898696899414 \n",
      "\n",
      "Batch 89/134 loss : 19.679563522338867 \n",
      "\n",
      "Batch 90/134 loss : 21.38411521911621 \n",
      "\n",
      "Batch 91/134 loss : 23.20226287841797 \n",
      "\n",
      "Batch 92/134 loss : 17.031782150268555 \n",
      "\n",
      "Batch 93/134 loss : 14.921481132507324 \n",
      "\n",
      "Batch 94/134 loss : 18.894657135009766 \n",
      "\n",
      "Batch 95/134 loss : 25.348875045776367 \n",
      "\n",
      "Batch 96/134 loss : 16.812225341796875 \n",
      "\n",
      "Batch 97/134 loss : 14.308708190917969 \n",
      "\n",
      "Batch 98/134 loss : 26.051597595214844 \n",
      "\n",
      "Batch 99/134 loss : 27.036396026611328 \n",
      "\n",
      "Batch 100/134 loss : 15.499166488647461 \n",
      "\n",
      "Batch 101/134 loss : 24.134788513183594 \n",
      "\n",
      "Batch 102/134 loss : 9.991439819335938 \n",
      "\n",
      "Batch 103/134 loss : 22.730403900146484 \n",
      "\n",
      "Batch 104/134 loss : 26.401485443115234 \n",
      "\n",
      "Batch 105/134 loss : 24.469022750854492 \n",
      "\n",
      "Batch 106/134 loss : 22.094213485717773 \n",
      "\n",
      "Batch 107/134 loss : 13.387079238891602 \n",
      "\n",
      "Batch 108/134 loss : 21.719247817993164 \n",
      "\n",
      "Batch 109/134 loss : 19.605690002441406 \n",
      "\n",
      "Batch 110/134 loss : 16.59827995300293 \n",
      "\n",
      "Batch 111/134 loss : 21.050424575805664 \n",
      "\n",
      "Batch 112/134 loss : 19.14191436767578 \n",
      "\n",
      "Batch 113/134 loss : 15.849998474121094 \n",
      "\n",
      "Batch 114/134 loss : 20.300464630126953 \n",
      "\n",
      "Batch 115/134 loss : 20.02910804748535 \n",
      "\n",
      "Batch 116/134 loss : 23.433382034301758 \n",
      "\n",
      "Batch 117/134 loss : 21.114986419677734 \n",
      "\n",
      "Batch 118/134 loss : 16.980466842651367 \n",
      "\n",
      "Batch 119/134 loss : 22.758419036865234 \n",
      "\n",
      "Batch 120/134 loss : 21.74689292907715 \n",
      "\n",
      "Batch 121/134 loss : 11.034790992736816 \n",
      "\n",
      "Batch 122/134 loss : 25.631324768066406 \n",
      "\n",
      "Batch 123/134 loss : 16.3309383392334 \n",
      "\n",
      "Batch 124/134 loss : 17.706987380981445 \n",
      "\n",
      "Batch 125/134 loss : 22.482038497924805 \n",
      "\n",
      "Batch 126/134 loss : 20.66577911376953 \n",
      "\n",
      "Batch 127/134 loss : 21.173965454101562 \n",
      "\n",
      "Batch 128/134 loss : 16.11620330810547 \n",
      "\n",
      "Batch 129/134 loss : 24.03588104248047 \n",
      "\n",
      "Batch 130/134 loss : 21.342723846435547 \n",
      "\n",
      "Batch 131/134 loss : 23.805866241455078 \n",
      "\n",
      "Batch 132/134 loss : 18.87574005126953 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 133/134 loss : 8.49421215057373 \n",
      "\n",
      "========== Epoch 3 ==========\n",
      "Batch 0/134 loss : 16.45189094543457 \n",
      "\n",
      "Batch 1/134 loss : 14.671704292297363 \n",
      "\n",
      "Batch 2/134 loss : 18.693368911743164 \n",
      "\n",
      "Batch 3/134 loss : 24.414857864379883 \n",
      "\n",
      "Batch 4/134 loss : 20.07969856262207 \n",
      "\n",
      "Batch 5/134 loss : 17.818195343017578 \n",
      "\n",
      "Batch 6/134 loss : 24.972721099853516 \n",
      "\n",
      "Batch 7/134 loss : 23.24526023864746 \n",
      "\n",
      "Batch 8/134 loss : 24.157817840576172 \n",
      "\n",
      "Batch 9/134 loss : 22.384754180908203 \n",
      "\n",
      "Batch 10/134 loss : 13.957990646362305 \n",
      "\n",
      "Batch 11/134 loss : 15.059709548950195 \n",
      "\n",
      "Batch 12/134 loss : 20.675907135009766 \n",
      "\n",
      "Batch 13/134 loss : 19.33056640625 \n",
      "\n",
      "Batch 14/134 loss : 23.417299270629883 \n",
      "\n",
      "Batch 15/134 loss : 17.35731315612793 \n",
      "\n",
      "Batch 16/134 loss : 18.26557159423828 \n",
      "\n",
      "Batch 17/134 loss : 18.106096267700195 \n",
      "\n",
      "Batch 18/134 loss : 14.742610931396484 \n",
      "\n",
      "Batch 19/134 loss : 12.089473724365234 \n",
      "\n",
      "Batch 20/134 loss : 18.889217376708984 \n",
      "\n",
      "Batch 21/134 loss : 19.078227996826172 \n",
      "\n",
      "Batch 22/134 loss : 15.531713485717773 \n",
      "\n",
      "Batch 23/134 loss : 22.124526977539062 \n",
      "\n",
      "Batch 24/134 loss : 25.88330078125 \n",
      "\n",
      "Batch 25/134 loss : 27.119064331054688 \n",
      "\n",
      "Batch 26/134 loss : 14.611970901489258 \n",
      "\n",
      "Batch 27/134 loss : 17.25588035583496 \n",
      "\n",
      "Batch 28/134 loss : 25.745601654052734 \n",
      "\n",
      "Batch 29/134 loss : 15.944387435913086 \n",
      "\n",
      "Batch 30/134 loss : 22.899511337280273 \n",
      "\n",
      "Batch 31/134 loss : 21.13404655456543 \n",
      "\n",
      "Batch 32/134 loss : 25.65768051147461 \n",
      "\n",
      "Batch 33/134 loss : 16.1246395111084 \n",
      "\n",
      "Batch 34/134 loss : 17.17076873779297 \n",
      "\n",
      "Batch 35/134 loss : 17.733097076416016 \n",
      "\n",
      "Batch 36/134 loss : 16.6961669921875 \n",
      "\n",
      "Batch 37/134 loss : 20.98596954345703 \n",
      "\n",
      "Batch 38/134 loss : 23.610294342041016 \n",
      "\n",
      "Batch 39/134 loss : 14.379975318908691 \n",
      "\n",
      "Batch 40/134 loss : 19.143199920654297 \n",
      "\n",
      "Batch 41/134 loss : 15.83237075805664 \n",
      "\n",
      "Batch 42/134 loss : 23.646770477294922 \n",
      "\n",
      "Batch 43/134 loss : 15.110818862915039 \n",
      "\n",
      "Batch 44/134 loss : 12.641202926635742 \n",
      "\n",
      "Batch 45/134 loss : 16.62538719177246 \n",
      "\n",
      "Batch 46/134 loss : 16.440446853637695 \n",
      "\n",
      "Batch 47/134 loss : 18.95472526550293 \n",
      "\n",
      "Batch 48/134 loss : 11.539702415466309 \n",
      "\n",
      "Batch 49/134 loss : 22.66604995727539 \n",
      "\n",
      "Batch 50/134 loss : 13.0296630859375 \n",
      "\n",
      "Batch 51/134 loss : 19.528608322143555 \n",
      "\n",
      "Batch 52/134 loss : 20.52735137939453 \n",
      "\n",
      "Batch 53/134 loss : 16.649106979370117 \n",
      "\n",
      "Batch 54/134 loss : 18.55902671813965 \n",
      "\n",
      "Batch 55/134 loss : 20.98082733154297 \n",
      "\n",
      "Batch 56/134 loss : 20.680143356323242 \n",
      "\n",
      "Batch 57/134 loss : 14.568608283996582 \n",
      "\n",
      "Batch 58/134 loss : 20.470069885253906 \n",
      "\n",
      "Batch 59/134 loss : 18.017871856689453 \n",
      "\n",
      "Batch 60/134 loss : 19.724609375 \n",
      "\n",
      "Batch 61/134 loss : 11.131114959716797 \n",
      "\n",
      "Batch 62/134 loss : 18.567760467529297 \n",
      "\n",
      "Batch 63/134 loss : 15.77502727508545 \n",
      "\n",
      "Batch 64/134 loss : 20.774456024169922 \n",
      "\n",
      "Batch 65/134 loss : 19.492122650146484 \n",
      "\n",
      "Batch 66/134 loss : 24.15876579284668 \n",
      "\n",
      "Batch 67/134 loss : 20.576763153076172 \n",
      "\n",
      "Batch 68/134 loss : 17.198612213134766 \n",
      "\n",
      "Batch 69/134 loss : 18.09437370300293 \n",
      "\n",
      "Batch 70/134 loss : 18.560131072998047 \n",
      "\n",
      "Batch 71/134 loss : 17.058956146240234 \n",
      "\n",
      "Batch 72/134 loss : 17.209718704223633 \n",
      "\n",
      "Batch 73/134 loss : 14.684953689575195 \n",
      "\n",
      "Batch 74/134 loss : 20.138389587402344 \n",
      "\n",
      "Batch 75/134 loss : 14.050740242004395 \n",
      "\n",
      "Batch 76/134 loss : 14.692243576049805 \n",
      "\n",
      "Batch 77/134 loss : 15.106413841247559 \n",
      "\n",
      "Batch 78/134 loss : 20.08367156982422 \n",
      "\n",
      "Batch 79/134 loss : 24.43012237548828 \n",
      "\n",
      "Batch 80/134 loss : 18.58909797668457 \n",
      "\n",
      "Batch 81/134 loss : 15.894787788391113 \n",
      "\n",
      "Batch 82/134 loss : 20.05591583251953 \n",
      "\n",
      "Batch 83/134 loss : 12.822356224060059 \n",
      "\n",
      "Batch 84/134 loss : 17.16167449951172 \n",
      "\n",
      "Batch 85/134 loss : 19.649410247802734 \n",
      "\n",
      "Batch 86/134 loss : 20.409257888793945 \n",
      "\n",
      "Batch 87/134 loss : 17.86367416381836 \n",
      "\n",
      "Batch 88/134 loss : 17.095998764038086 \n",
      "\n",
      "Batch 89/134 loss : 14.647005081176758 \n",
      "\n",
      "Batch 90/134 loss : 18.42974853515625 \n",
      "\n",
      "Batch 91/134 loss : 16.540428161621094 \n",
      "\n",
      "Batch 92/134 loss : 19.527925491333008 \n",
      "\n",
      "Batch 93/134 loss : 18.114059448242188 \n",
      "\n",
      "Batch 94/134 loss : 16.955703735351562 \n",
      "\n",
      "Batch 95/134 loss : 21.973255157470703 \n",
      "\n",
      "Batch 96/134 loss : 21.498172760009766 \n",
      "\n",
      "Batch 97/134 loss : 17.317035675048828 \n",
      "\n",
      "Batch 98/134 loss : 23.568334579467773 \n",
      "\n",
      "Batch 99/134 loss : 16.389719009399414 \n",
      "\n",
      "Batch 100/134 loss : 22.03399085998535 \n",
      "\n",
      "Batch 101/134 loss : 15.88137435913086 \n",
      "\n",
      "Batch 102/134 loss : 16.67781639099121 \n",
      "\n",
      "Batch 103/134 loss : 15.134965896606445 \n",
      "\n",
      "Batch 104/134 loss : 24.282093048095703 \n",
      "\n",
      "Batch 105/134 loss : 17.21065902709961 \n",
      "\n",
      "Batch 106/134 loss : 17.81256675720215 \n",
      "\n",
      "Batch 107/134 loss : 10.847175598144531 \n",
      "\n",
      "Batch 108/134 loss : 16.04215431213379 \n",
      "\n",
      "Batch 109/134 loss : 13.479175567626953 \n",
      "\n",
      "Batch 110/134 loss : 14.874422073364258 \n",
      "\n",
      "Batch 111/134 loss : 23.193090438842773 \n",
      "\n",
      "Batch 112/134 loss : 21.583520889282227 \n",
      "\n",
      "Batch 113/134 loss : 15.51760482788086 \n",
      "\n",
      "Batch 114/134 loss : 8.813111305236816 \n",
      "\n",
      "Batch 115/134 loss : 9.506954193115234 \n",
      "\n",
      "Batch 116/134 loss : 18.465957641601562 \n",
      "\n",
      "Batch 117/134 loss : 17.38185691833496 \n",
      "\n",
      "Batch 118/134 loss : 24.914079666137695 \n",
      "\n",
      "Batch 119/134 loss : 15.343731880187988 \n",
      "\n",
      "Batch 120/134 loss : 20.461536407470703 \n",
      "\n",
      "Batch 121/134 loss : 11.964128494262695 \n",
      "\n",
      "Batch 122/134 loss : 18.80278968811035 \n",
      "\n",
      "Batch 123/134 loss : 15.088190078735352 \n",
      "\n",
      "Batch 124/134 loss : 11.995525360107422 \n",
      "\n",
      "Batch 125/134 loss : 23.031478881835938 \n",
      "\n",
      "Batch 126/134 loss : 22.19097137451172 \n",
      "\n",
      "Batch 127/134 loss : 21.842275619506836 \n",
      "\n",
      "Batch 128/134 loss : 17.39983367919922 \n",
      "\n",
      "Batch 129/134 loss : 15.308655738830566 \n",
      "\n",
      "Batch 130/134 loss : 16.781448364257812 \n",
      "\n",
      "Batch 131/134 loss : 14.936748504638672 \n",
      "\n",
      "Batch 132/134 loss : 13.58318042755127 \n",
      "\n",
      "Batch 133/134 loss : 30.84564971923828 \n",
      "\n",
      "========== Epoch 4 ==========\n",
      "Batch 0/134 loss : 17.343955993652344 \n",
      "\n",
      "Batch 1/134 loss : 22.965269088745117 \n",
      "\n",
      "Batch 2/134 loss : 21.08057403564453 \n",
      "\n",
      "Batch 3/134 loss : 19.073209762573242 \n",
      "\n",
      "Batch 4/134 loss : 17.39666175842285 \n",
      "\n",
      "Batch 5/134 loss : 23.44348907470703 \n",
      "\n",
      "Batch 6/134 loss : 21.5695743560791 \n",
      "\n",
      "Batch 7/134 loss : 12.506988525390625 \n",
      "\n",
      "Batch 8/134 loss : 15.482158660888672 \n",
      "\n",
      "Batch 9/134 loss : 17.809736251831055 \n",
      "\n",
      "Batch 10/134 loss : 15.292325973510742 \n",
      "\n",
      "Batch 11/134 loss : 22.258712768554688 \n",
      "\n",
      "Batch 12/134 loss : 17.300413131713867 \n",
      "\n",
      "Batch 13/134 loss : 18.801467895507812 \n",
      "\n",
      "Batch 14/134 loss : 23.193191528320312 \n",
      "\n",
      "Batch 15/134 loss : 19.023273468017578 \n",
      "\n",
      "Batch 16/134 loss : 17.113224029541016 \n",
      "\n",
      "Batch 17/134 loss : 25.056697845458984 \n",
      "\n",
      "Batch 18/134 loss : 22.100936889648438 \n",
      "\n",
      "Batch 19/134 loss : 17.471988677978516 \n",
      "\n",
      "Batch 20/134 loss : 14.758822441101074 \n",
      "\n",
      "Batch 21/134 loss : 24.647428512573242 \n",
      "\n",
      "Batch 22/134 loss : 23.43516731262207 \n",
      "\n",
      "Batch 23/134 loss : 15.768661499023438 \n",
      "\n",
      "Batch 24/134 loss : 15.488067626953125 \n",
      "\n",
      "Batch 25/134 loss : 11.268148422241211 \n",
      "\n",
      "Batch 26/134 loss : 20.213790893554688 \n",
      "\n",
      "Batch 27/134 loss : 22.600507736206055 \n",
      "\n",
      "Batch 28/134 loss : 19.80205726623535 \n",
      "\n",
      "Batch 29/134 loss : 14.24935531616211 \n",
      "\n",
      "Batch 30/134 loss : 20.60750961303711 \n",
      "\n",
      "Batch 31/134 loss : 19.875473022460938 \n",
      "\n",
      "Batch 32/134 loss : 18.616153717041016 \n",
      "\n",
      "Batch 33/134 loss : 15.093385696411133 \n",
      "\n",
      "Batch 34/134 loss : 15.889375686645508 \n",
      "\n",
      "Batch 35/134 loss : 21.521318435668945 \n",
      "\n",
      "Batch 36/134 loss : 22.75000762939453 \n",
      "\n",
      "Batch 37/134 loss : 21.89394187927246 \n",
      "\n",
      "Batch 38/134 loss : 19.249364852905273 \n",
      "\n",
      "Batch 39/134 loss : 14.443564414978027 \n",
      "\n",
      "Batch 40/134 loss : 12.169084548950195 \n",
      "\n",
      "Batch 41/134 loss : 15.39183235168457 \n",
      "\n",
      "Batch 42/134 loss : 14.370420455932617 \n",
      "\n",
      "Batch 43/134 loss : 21.605514526367188 \n",
      "\n",
      "Batch 44/134 loss : 22.485889434814453 \n",
      "\n",
      "Batch 45/134 loss : 17.940868377685547 \n",
      "\n",
      "Batch 46/134 loss : 14.737993240356445 \n",
      "\n",
      "Batch 47/134 loss : 14.600019454956055 \n",
      "\n",
      "Batch 48/134 loss : 23.44862174987793 \n",
      "\n",
      "Batch 49/134 loss : 21.8939208984375 \n",
      "\n",
      "Batch 50/134 loss : 20.57087516784668 \n",
      "\n",
      "Batch 51/134 loss : 18.81612777709961 \n",
      "\n",
      "Batch 52/134 loss : 15.785140991210938 \n",
      "\n",
      "Batch 53/134 loss : 24.817827224731445 \n",
      "\n",
      "Batch 54/134 loss : 20.546058654785156 \n",
      "\n",
      "Batch 55/134 loss : 12.246420860290527 \n",
      "\n",
      "Batch 56/134 loss : 21.245853424072266 \n",
      "\n",
      "Batch 57/134 loss : 20.05021095275879 \n",
      "\n",
      "Batch 58/134 loss : 12.238061904907227 \n",
      "\n",
      "Batch 59/134 loss : 16.736671447753906 \n",
      "\n",
      "Batch 60/134 loss : 21.03162956237793 \n",
      "\n",
      "Batch 61/134 loss : 18.873676300048828 \n",
      "\n",
      "Batch 62/134 loss : 21.9821834564209 \n",
      "\n",
      "Batch 63/134 loss : 23.391538619995117 \n",
      "\n",
      "Batch 64/134 loss : 20.305959701538086 \n",
      "\n",
      "Batch 65/134 loss : 11.396618843078613 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 66/134 loss : 16.260038375854492 \n",
      "\n",
      "Batch 67/134 loss : 16.27086067199707 \n",
      "\n",
      "Batch 68/134 loss : 17.829591751098633 \n",
      "\n",
      "Batch 69/134 loss : 18.26114845275879 \n",
      "\n",
      "Batch 70/134 loss : 15.348941802978516 \n",
      "\n",
      "Batch 71/134 loss : 14.364439010620117 \n",
      "\n",
      "Batch 72/134 loss : 13.881126403808594 \n",
      "\n",
      "Batch 73/134 loss : 19.628761291503906 \n",
      "\n",
      "Batch 74/134 loss : 20.511966705322266 \n",
      "\n",
      "Batch 75/134 loss : 18.758636474609375 \n",
      "\n",
      "Batch 76/134 loss : 17.48552703857422 \n",
      "\n",
      "Batch 77/134 loss : 13.222464561462402 \n",
      "\n",
      "Batch 78/134 loss : 20.886417388916016 \n",
      "\n",
      "Batch 79/134 loss : 14.93856143951416 \n",
      "\n",
      "Batch 80/134 loss : 12.074054718017578 \n",
      "\n",
      "Batch 81/134 loss : 12.079110145568848 \n",
      "\n",
      "Batch 82/134 loss : 12.345052719116211 \n",
      "\n",
      "Batch 83/134 loss : 12.26734447479248 \n",
      "\n",
      "Batch 84/134 loss : 18.205812454223633 \n",
      "\n",
      "Batch 85/134 loss : 15.108814239501953 \n",
      "\n",
      "Batch 86/134 loss : 11.76219367980957 \n",
      "\n",
      "Batch 87/134 loss : 10.594650268554688 \n",
      "\n",
      "Batch 88/134 loss : 17.10917091369629 \n",
      "\n",
      "Batch 89/134 loss : 16.884273529052734 \n",
      "\n",
      "Batch 90/134 loss : 17.24742889404297 \n",
      "\n",
      "Batch 91/134 loss : 18.20969581604004 \n",
      "\n",
      "Batch 92/134 loss : 16.80403709411621 \n",
      "\n",
      "Batch 93/134 loss : 10.965779304504395 \n",
      "\n",
      "Batch 94/134 loss : 16.963031768798828 \n",
      "\n",
      "Batch 95/134 loss : 14.992101669311523 \n",
      "\n",
      "Batch 96/134 loss : 17.918285369873047 \n",
      "\n",
      "Batch 97/134 loss : 11.663521766662598 \n",
      "\n",
      "Batch 98/134 loss : 12.596824645996094 \n",
      "\n",
      "Batch 99/134 loss : 18.66135597229004 \n",
      "\n",
      "Batch 100/134 loss : 14.793621063232422 \n",
      "\n",
      "Batch 101/134 loss : 19.54680633544922 \n",
      "\n",
      "Batch 102/134 loss : 16.226978302001953 \n",
      "\n",
      "Batch 103/134 loss : 13.523411750793457 \n",
      "\n",
      "Batch 104/134 loss : 14.192392349243164 \n",
      "\n",
      "Batch 105/134 loss : 19.97971534729004 \n",
      "\n",
      "Batch 106/134 loss : 18.839221954345703 \n",
      "\n",
      "Batch 107/134 loss : 25.743911743164062 \n",
      "\n",
      "Batch 108/134 loss : 14.913427352905273 \n",
      "\n",
      "Batch 109/134 loss : 24.513246536254883 \n",
      "\n",
      "Batch 110/134 loss : 17.08540153503418 \n",
      "\n",
      "Batch 111/134 loss : 14.809678077697754 \n",
      "\n",
      "Batch 112/134 loss : 20.63178825378418 \n",
      "\n",
      "Batch 113/134 loss : 17.980243682861328 \n",
      "\n",
      "Batch 114/134 loss : 17.742935180664062 \n",
      "\n",
      "Batch 115/134 loss : 12.364799499511719 \n",
      "\n",
      "Batch 116/134 loss : 17.153987884521484 \n",
      "\n",
      "Batch 117/134 loss : 12.116557121276855 \n",
      "\n",
      "Batch 118/134 loss : 19.058332443237305 \n",
      "\n",
      "Batch 119/134 loss : 21.01799964904785 \n",
      "\n",
      "Batch 120/134 loss : 13.774943351745605 \n",
      "\n",
      "Batch 121/134 loss : 12.171920776367188 \n",
      "\n",
      "Batch 122/134 loss : 15.697489738464355 \n",
      "\n",
      "Batch 123/134 loss : 12.141599655151367 \n",
      "\n",
      "Batch 124/134 loss : 19.733375549316406 \n",
      "\n",
      "Batch 125/134 loss : 7.377534866333008 \n",
      "\n",
      "Batch 126/134 loss : 16.25847816467285 \n",
      "\n",
      "Batch 127/134 loss : 16.989181518554688 \n",
      "\n",
      "Batch 128/134 loss : 13.349669456481934 \n",
      "\n",
      "Batch 129/134 loss : 18.915569305419922 \n",
      "\n",
      "Batch 130/134 loss : 11.272229194641113 \n",
      "\n",
      "Batch 131/134 loss : 15.167436599731445 \n",
      "\n",
      "Batch 132/134 loss : 12.877026557922363 \n",
      "\n",
      "Batch 133/134 loss : 17.469511032104492 \n",
      "\n",
      "========== Epoch 5 ==========\n",
      "Batch 0/134 loss : 16.740751266479492 \n",
      "\n",
      "Batch 1/134 loss : 17.35483169555664 \n",
      "\n",
      "Batch 2/134 loss : 10.599287033081055 \n",
      "\n",
      "Batch 3/134 loss : 14.063640594482422 \n",
      "\n",
      "Batch 4/134 loss : 16.26704216003418 \n",
      "\n",
      "Batch 5/134 loss : 14.364602088928223 \n",
      "\n",
      "Batch 6/134 loss : 19.26133918762207 \n",
      "\n",
      "Batch 7/134 loss : 16.32881736755371 \n",
      "\n",
      "Batch 8/134 loss : 27.93504524230957 \n",
      "\n",
      "Batch 9/134 loss : 22.30959129333496 \n",
      "\n",
      "Batch 10/134 loss : 13.520452499389648 \n",
      "\n",
      "Batch 11/134 loss : 16.589523315429688 \n",
      "\n",
      "Batch 12/134 loss : 18.91299819946289 \n",
      "\n",
      "Batch 13/134 loss : 19.81632423400879 \n",
      "\n",
      "Batch 14/134 loss : 19.411354064941406 \n",
      "\n",
      "Batch 15/134 loss : 17.398075103759766 \n",
      "\n",
      "Batch 16/134 loss : 16.05937385559082 \n",
      "\n",
      "Batch 17/134 loss : 26.178058624267578 \n",
      "\n",
      "Batch 18/134 loss : 18.696399688720703 \n",
      "\n",
      "Batch 19/134 loss : 19.422517776489258 \n",
      "\n",
      "Batch 20/134 loss : 21.601499557495117 \n",
      "\n",
      "Batch 21/134 loss : 18.1685848236084 \n",
      "\n",
      "Batch 22/134 loss : 11.03629207611084 \n",
      "\n",
      "Batch 23/134 loss : 15.249336242675781 \n",
      "\n",
      "Batch 24/134 loss : 21.13595199584961 \n",
      "\n",
      "Batch 25/134 loss : 15.832019805908203 \n",
      "\n",
      "Batch 26/134 loss : 21.860464096069336 \n",
      "\n",
      "Batch 27/134 loss : 15.77163314819336 \n",
      "\n",
      "Batch 28/134 loss : 14.552969932556152 \n",
      "\n",
      "Batch 29/134 loss : 15.4677734375 \n",
      "\n",
      "Batch 30/134 loss : 18.547576904296875 \n",
      "\n",
      "Batch 31/134 loss : 16.17185401916504 \n",
      "\n",
      "Batch 32/134 loss : 7.113874435424805 \n",
      "\n",
      "Batch 33/134 loss : 15.019248962402344 \n",
      "\n",
      "Batch 34/134 loss : 19.87542152404785 \n",
      "\n",
      "Batch 35/134 loss : 14.069158554077148 \n",
      "\n",
      "Batch 36/134 loss : 22.27065086364746 \n",
      "\n",
      "Batch 37/134 loss : 14.106413841247559 \n",
      "\n",
      "Batch 38/134 loss : 21.717554092407227 \n",
      "\n",
      "Batch 39/134 loss : 8.676589965820312 \n",
      "\n",
      "Batch 40/134 loss : 15.38193130493164 \n",
      "\n",
      "Batch 41/134 loss : 9.493796348571777 \n",
      "\n",
      "Batch 42/134 loss : 8.733458518981934 \n",
      "\n",
      "Batch 43/134 loss : 18.412818908691406 \n",
      "\n",
      "Batch 44/134 loss : 17.43829345703125 \n",
      "\n",
      "Batch 45/134 loss : 14.70639419555664 \n",
      "\n",
      "Batch 46/134 loss : 18.599885940551758 \n",
      "\n",
      "Batch 47/134 loss : 14.813572883605957 \n",
      "\n",
      "Batch 48/134 loss : 14.8233003616333 \n",
      "\n",
      "Batch 49/134 loss : 15.556809425354004 \n",
      "\n",
      "Batch 50/134 loss : 15.07825756072998 \n",
      "\n",
      "Batch 51/134 loss : 10.91365909576416 \n",
      "\n",
      "Batch 52/134 loss : 17.387033462524414 \n",
      "\n",
      "Batch 53/134 loss : 15.667558670043945 \n",
      "\n",
      "Batch 54/134 loss : 14.346968650817871 \n",
      "\n",
      "Batch 55/134 loss : 11.46971607208252 \n",
      "\n",
      "Batch 56/134 loss : 13.776115417480469 \n",
      "\n",
      "Batch 57/134 loss : 15.073927879333496 \n",
      "\n",
      "Batch 58/134 loss : 8.213850021362305 \n",
      "\n",
      "Batch 59/134 loss : 6.424468040466309 \n",
      "\n",
      "Batch 60/134 loss : 11.637457847595215 \n",
      "\n",
      "Batch 61/134 loss : 12.974398612976074 \n",
      "\n",
      "Batch 62/134 loss : 20.965259552001953 \n",
      "\n",
      "Batch 63/134 loss : 16.577144622802734 \n",
      "\n",
      "Batch 64/134 loss : 14.939779281616211 \n",
      "\n",
      "Batch 65/134 loss : 19.42449951171875 \n",
      "\n",
      "Batch 66/134 loss : 13.069416999816895 \n",
      "\n",
      "Batch 67/134 loss : 17.697269439697266 \n",
      "\n",
      "Batch 68/134 loss : 20.373563766479492 \n",
      "\n",
      "Batch 69/134 loss : 11.773090362548828 \n",
      "\n",
      "Batch 70/134 loss : 7.659963607788086 \n",
      "\n",
      "Batch 71/134 loss : 17.0988712310791 \n",
      "\n",
      "Batch 72/134 loss : 16.7807674407959 \n",
      "\n",
      "Batch 73/134 loss : 19.955705642700195 \n",
      "\n",
      "Batch 74/134 loss : 9.25574779510498 \n",
      "\n",
      "Batch 75/134 loss : 20.817920684814453 \n",
      "\n",
      "Batch 76/134 loss : 14.259797096252441 \n",
      "\n",
      "Batch 77/134 loss : 17.629776000976562 \n",
      "\n",
      "Batch 78/134 loss : 16.802932739257812 \n",
      "\n",
      "Batch 79/134 loss : 10.17911434173584 \n",
      "\n",
      "Batch 80/134 loss : 16.14128303527832 \n",
      "\n",
      "Batch 81/134 loss : 17.091110229492188 \n",
      "\n",
      "Batch 82/134 loss : 9.898340225219727 \n",
      "\n",
      "Batch 83/134 loss : 10.073223114013672 \n",
      "\n",
      "Batch 84/134 loss : 14.604992866516113 \n",
      "\n",
      "Batch 85/134 loss : 13.600083351135254 \n",
      "\n",
      "Batch 86/134 loss : 11.607237815856934 \n",
      "\n",
      "Batch 87/134 loss : 10.483888626098633 \n",
      "\n",
      "Batch 88/134 loss : 21.23662757873535 \n",
      "\n",
      "Batch 89/134 loss : 11.084623336791992 \n",
      "\n",
      "Batch 90/134 loss : 16.542104721069336 \n",
      "\n",
      "Batch 91/134 loss : 19.681228637695312 \n",
      "\n",
      "Batch 92/134 loss : 14.665009498596191 \n",
      "\n",
      "Batch 93/134 loss : 17.368925094604492 \n",
      "\n",
      "Batch 94/134 loss : 14.974611282348633 \n",
      "\n",
      "Batch 95/134 loss : 9.365152359008789 \n",
      "\n",
      "Batch 96/134 loss : 15.955320358276367 \n",
      "\n",
      "Batch 97/134 loss : 14.212441444396973 \n",
      "\n",
      "Batch 98/134 loss : 16.01617431640625 \n",
      "\n",
      "Batch 99/134 loss : 16.914337158203125 \n",
      "\n",
      "Batch 100/134 loss : 19.953989028930664 \n",
      "\n",
      "Batch 101/134 loss : 21.15599822998047 \n",
      "\n",
      "Batch 102/134 loss : 16.27806854248047 \n",
      "\n",
      "Batch 103/134 loss : 18.90215301513672 \n",
      "\n",
      "Batch 104/134 loss : 16.0162296295166 \n",
      "\n",
      "Batch 105/134 loss : 14.76805305480957 \n",
      "\n",
      "Batch 106/134 loss : 15.371892929077148 \n",
      "\n",
      "Batch 107/134 loss : 13.607410430908203 \n",
      "\n",
      "Batch 108/134 loss : 15.21099853515625 \n",
      "\n",
      "Batch 109/134 loss : 16.854787826538086 \n",
      "\n",
      "Batch 110/134 loss : 10.032135963439941 \n",
      "\n",
      "Batch 111/134 loss : 11.472492218017578 \n",
      "\n",
      "Batch 112/134 loss : 13.598231315612793 \n",
      "\n",
      "Batch 113/134 loss : 12.599580764770508 \n",
      "\n",
      "Batch 114/134 loss : 18.031648635864258 \n",
      "\n",
      "Batch 115/134 loss : 10.473464965820312 \n",
      "\n",
      "Batch 116/134 loss : 12.701282501220703 \n",
      "\n",
      "Batch 117/134 loss : 13.324861526489258 \n",
      "\n",
      "Batch 118/134 loss : 17.057527542114258 \n",
      "\n",
      "Batch 119/134 loss : 16.419597625732422 \n",
      "\n",
      "Batch 120/134 loss : 20.5386962890625 \n",
      "\n",
      "Batch 121/134 loss : 12.129719734191895 \n",
      "\n",
      "Batch 122/134 loss : 21.02008628845215 \n",
      "\n",
      "Batch 123/134 loss : 17.180980682373047 \n",
      "\n",
      "Batch 124/134 loss : 21.8801212310791 \n",
      "\n",
      "Batch 125/134 loss : 12.688496589660645 \n",
      "\n",
      "Batch 126/134 loss : 10.004034042358398 \n",
      "\n",
      "Batch 127/134 loss : 21.27281951904297 \n",
      "\n",
      "Batch 128/134 loss : 19.9277400970459 \n",
      "\n",
      "Batch 129/134 loss : 16.301231384277344 \n",
      "\n",
      "Batch 130/134 loss : 17.12401580810547 \n",
      "\n",
      "Batch 131/134 loss : 15.572091102600098 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 132/134 loss : 15.052877426147461 \n",
      "\n",
      "Batch 133/134 loss : 48.1066780090332 \n",
      "\n",
      "========== Epoch 6 ==========\n",
      "Batch 0/134 loss : 21.517038345336914 \n",
      "\n",
      "Batch 1/134 loss : 17.292255401611328 \n",
      "\n",
      "Batch 2/134 loss : 10.30292797088623 \n",
      "\n",
      "Batch 3/134 loss : 19.324617385864258 \n",
      "\n",
      "Batch 4/134 loss : 17.816118240356445 \n",
      "\n",
      "Batch 5/134 loss : 14.459976196289062 \n",
      "\n",
      "Batch 6/134 loss : 12.628499984741211 \n",
      "\n",
      "Batch 7/134 loss : 11.434514999389648 \n",
      "\n",
      "Batch 8/134 loss : 16.604598999023438 \n",
      "\n",
      "Batch 9/134 loss : 14.61286735534668 \n",
      "\n",
      "Batch 10/134 loss : 12.126258850097656 \n",
      "\n",
      "Batch 11/134 loss : 12.385948181152344 \n",
      "\n",
      "Batch 12/134 loss : 11.945281028747559 \n",
      "\n",
      "Batch 13/134 loss : 11.886029243469238 \n",
      "\n",
      "Batch 14/134 loss : 13.66872787475586 \n",
      "\n",
      "Batch 15/134 loss : 15.994502067565918 \n",
      "\n",
      "Batch 16/134 loss : 20.885480880737305 \n",
      "\n",
      "Batch 17/134 loss : 16.431148529052734 \n",
      "\n",
      "Batch 18/134 loss : 20.35764503479004 \n",
      "\n",
      "Batch 19/134 loss : 15.863480567932129 \n",
      "\n",
      "Batch 20/134 loss : 13.84191608428955 \n",
      "\n",
      "Batch 21/134 loss : 14.525917053222656 \n",
      "\n",
      "Batch 22/134 loss : 17.22479248046875 \n",
      "\n",
      "Batch 23/134 loss : 15.849769592285156 \n",
      "\n",
      "Batch 24/134 loss : 18.73996925354004 \n",
      "\n",
      "Batch 25/134 loss : 14.024190902709961 \n",
      "\n",
      "Batch 26/134 loss : 11.949775695800781 \n",
      "\n",
      "Batch 27/134 loss : 19.591527938842773 \n",
      "\n",
      "Batch 28/134 loss : 16.520044326782227 \n",
      "\n",
      "Batch 29/134 loss : 16.35121726989746 \n",
      "\n",
      "Batch 30/134 loss : 13.462767601013184 \n",
      "\n",
      "Batch 31/134 loss : 18.11473846435547 \n",
      "\n",
      "Batch 32/134 loss : 10.771777153015137 \n",
      "\n",
      "Batch 33/134 loss : 18.604475021362305 \n",
      "\n",
      "Batch 34/134 loss : 7.848038673400879 \n",
      "\n",
      "Batch 35/134 loss : 15.125658988952637 \n",
      "\n",
      "Batch 36/134 loss : 17.086923599243164 \n",
      "\n",
      "Batch 37/134 loss : 16.355133056640625 \n",
      "\n",
      "Batch 38/134 loss : 13.422948837280273 \n",
      "\n",
      "Batch 39/134 loss : 14.4266996383667 \n",
      "\n",
      "Batch 40/134 loss : 18.835773468017578 \n",
      "\n",
      "Batch 41/134 loss : 9.532524108886719 \n",
      "\n",
      "Batch 42/134 loss : 10.055253028869629 \n",
      "\n",
      "Batch 43/134 loss : 14.400760650634766 \n",
      "\n",
      "Batch 44/134 loss : 17.991317749023438 \n",
      "\n",
      "Batch 45/134 loss : 11.979615211486816 \n",
      "\n",
      "Batch 46/134 loss : 12.155647277832031 \n",
      "\n",
      "Batch 47/134 loss : 14.678252220153809 \n",
      "\n",
      "Batch 48/134 loss : 17.46727752685547 \n",
      "\n",
      "Batch 49/134 loss : 20.415117263793945 \n",
      "\n",
      "Batch 50/134 loss : 14.861413955688477 \n",
      "\n",
      "Batch 51/134 loss : 12.2771635055542 \n",
      "\n",
      "Batch 52/134 loss : 15.762596130371094 \n",
      "\n",
      "Batch 53/134 loss : 18.02377700805664 \n",
      "\n",
      "Batch 54/134 loss : 12.877277374267578 \n",
      "\n",
      "Batch 55/134 loss : 19.715286254882812 \n",
      "\n",
      "Batch 56/134 loss : 20.637773513793945 \n",
      "\n",
      "Batch 57/134 loss : 12.240333557128906 \n",
      "\n",
      "Batch 58/134 loss : 17.85875129699707 \n",
      "\n",
      "Batch 59/134 loss : 15.781024932861328 \n",
      "\n",
      "Batch 60/134 loss : 18.52944564819336 \n",
      "\n",
      "Batch 61/134 loss : 14.143911361694336 \n",
      "\n",
      "Batch 62/134 loss : 13.46505355834961 \n",
      "\n",
      "Batch 63/134 loss : 15.956101417541504 \n",
      "\n",
      "Batch 64/134 loss : 13.785703659057617 \n",
      "\n",
      "Batch 65/134 loss : 16.08902359008789 \n",
      "\n",
      "Batch 66/134 loss : 13.618904113769531 \n",
      "\n",
      "Batch 67/134 loss : 16.31394386291504 \n",
      "\n",
      "Batch 68/134 loss : 15.335468292236328 \n",
      "\n",
      "Batch 69/134 loss : 11.899561882019043 \n",
      "\n",
      "Batch 70/134 loss : 14.62707805633545 \n",
      "\n",
      "Batch 71/134 loss : 9.955351829528809 \n",
      "\n",
      "Batch 72/134 loss : 12.861462593078613 \n",
      "\n",
      "Batch 73/134 loss : 13.82742977142334 \n",
      "\n",
      "Batch 74/134 loss : 10.75743579864502 \n",
      "\n",
      "Batch 75/134 loss : 12.234569549560547 \n",
      "\n",
      "Batch 76/134 loss : 13.646788597106934 \n",
      "\n",
      "Batch 77/134 loss : 12.076602935791016 \n",
      "\n",
      "Batch 78/134 loss : 19.6307315826416 \n",
      "\n",
      "Batch 79/134 loss : 10.155678749084473 \n",
      "\n",
      "Batch 80/134 loss : 16.513690948486328 \n",
      "\n",
      "Batch 81/134 loss : 9.255030632019043 \n",
      "\n",
      "Batch 82/134 loss : 14.287309646606445 \n",
      "\n",
      "Batch 83/134 loss : 4.519067764282227 \n",
      "\n",
      "Batch 84/134 loss : 13.291155815124512 \n",
      "\n",
      "Batch 85/134 loss : 13.941984176635742 \n",
      "\n",
      "Batch 86/134 loss : 9.19430160522461 \n",
      "\n",
      "Batch 87/134 loss : 14.672114372253418 \n",
      "\n",
      "Batch 88/134 loss : 8.58884048461914 \n",
      "\n",
      "Batch 89/134 loss : 15.18185043334961 \n",
      "\n",
      "Batch 90/134 loss : 15.253847122192383 \n",
      "\n",
      "Batch 91/134 loss : 18.384151458740234 \n",
      "\n",
      "Batch 92/134 loss : 12.363195419311523 \n",
      "\n",
      "Batch 93/134 loss : 20.884124755859375 \n",
      "\n",
      "Batch 94/134 loss : 13.769491195678711 \n",
      "\n",
      "Batch 95/134 loss : 18.535995483398438 \n",
      "\n",
      "Batch 96/134 loss : 16.010528564453125 \n",
      "\n",
      "Batch 97/134 loss : 12.233222007751465 \n",
      "\n",
      "Batch 98/134 loss : 10.275753021240234 \n",
      "\n",
      "Batch 99/134 loss : 18.571455001831055 \n",
      "\n",
      "Batch 100/134 loss : 16.608266830444336 \n",
      "\n",
      "Batch 101/134 loss : 19.067781448364258 \n",
      "\n",
      "Batch 102/134 loss : 10.016351699829102 \n",
      "\n",
      "Batch 103/134 loss : 15.359834671020508 \n",
      "\n",
      "Batch 104/134 loss : 15.682073593139648 \n",
      "\n",
      "Batch 105/134 loss : 18.311296463012695 \n",
      "\n",
      "Batch 106/134 loss : 23.34625244140625 \n",
      "\n",
      "Batch 107/134 loss : 11.151470184326172 \n",
      "\n",
      "Batch 108/134 loss : 15.146259307861328 \n",
      "\n",
      "Batch 109/134 loss : 12.410665512084961 \n",
      "\n",
      "Batch 110/134 loss : 15.098531723022461 \n",
      "\n",
      "Batch 111/134 loss : 11.32680892944336 \n",
      "\n",
      "Batch 112/134 loss : 14.546664237976074 \n",
      "\n",
      "Batch 113/134 loss : 17.09235382080078 \n",
      "\n",
      "Batch 114/134 loss : 14.97806453704834 \n",
      "\n",
      "Batch 115/134 loss : 14.955524444580078 \n",
      "\n",
      "Batch 116/134 loss : 16.038549423217773 \n",
      "\n",
      "Batch 117/134 loss : 15.151701927185059 \n",
      "\n",
      "Batch 118/134 loss : 16.32857322692871 \n",
      "\n",
      "Batch 119/134 loss : 10.45281982421875 \n",
      "\n",
      "Batch 120/134 loss : 13.921942710876465 \n",
      "\n",
      "Batch 121/134 loss : 16.419387817382812 \n",
      "\n",
      "Batch 122/134 loss : 23.989215850830078 \n",
      "\n",
      "Batch 123/134 loss : 7.339022636413574 \n",
      "\n",
      "Batch 124/134 loss : 22.544368743896484 \n",
      "\n",
      "Batch 125/134 loss : 15.081374168395996 \n",
      "\n",
      "Batch 126/134 loss : 16.57994842529297 \n",
      "\n",
      "Batch 127/134 loss : 11.668551445007324 \n",
      "\n",
      "Batch 128/134 loss : 13.718984603881836 \n",
      "\n",
      "Batch 129/134 loss : 12.567936897277832 \n",
      "\n",
      "Batch 130/134 loss : 16.29084014892578 \n",
      "\n",
      "Batch 131/134 loss : 13.449417114257812 \n",
      "\n",
      "Batch 132/134 loss : 12.14661979675293 \n",
      "\n",
      "Batch 133/134 loss : 4.875877380371094 \n",
      "\n",
      "========== Epoch 7 ==========\n",
      "Batch 0/134 loss : 10.533210754394531 \n",
      "\n",
      "Batch 1/134 loss : 8.015984535217285 \n",
      "\n",
      "Batch 2/134 loss : 12.890514373779297 \n",
      "\n",
      "Batch 3/134 loss : 8.834482192993164 \n",
      "\n",
      "Batch 4/134 loss : 17.82672691345215 \n",
      "\n",
      "Batch 5/134 loss : 13.595097541809082 \n",
      "\n",
      "Batch 6/134 loss : 12.43714714050293 \n",
      "\n",
      "Batch 7/134 loss : 14.852384567260742 \n",
      "\n",
      "Batch 8/134 loss : 17.11463737487793 \n",
      "\n",
      "Batch 9/134 loss : 17.208911895751953 \n",
      "\n",
      "Batch 10/134 loss : 12.214612007141113 \n",
      "\n",
      "Batch 11/134 loss : 17.401155471801758 \n",
      "\n",
      "Batch 12/134 loss : 14.52895736694336 \n",
      "\n",
      "Batch 13/134 loss : 11.441987037658691 \n",
      "\n",
      "Batch 14/134 loss : 13.229662895202637 \n",
      "\n",
      "Batch 15/134 loss : 11.89046859741211 \n",
      "\n",
      "Batch 16/134 loss : 11.96165657043457 \n",
      "\n",
      "Batch 17/134 loss : 13.010910034179688 \n",
      "\n",
      "Batch 18/134 loss : 15.572094917297363 \n",
      "\n",
      "Batch 19/134 loss : 20.563331604003906 \n",
      "\n",
      "Batch 20/134 loss : 20.371509552001953 \n",
      "\n",
      "Batch 21/134 loss : 13.735517501831055 \n",
      "\n",
      "Batch 22/134 loss : 13.7180757522583 \n",
      "\n",
      "Batch 23/134 loss : 14.328531265258789 \n",
      "\n",
      "Batch 24/134 loss : 16.51671600341797 \n",
      "\n",
      "Batch 25/134 loss : 8.33824348449707 \n",
      "\n",
      "Batch 26/134 loss : 13.350180625915527 \n",
      "\n",
      "Batch 27/134 loss : 14.389935493469238 \n",
      "\n",
      "Batch 28/134 loss : 10.904168128967285 \n",
      "\n",
      "Batch 29/134 loss : 11.594831466674805 \n",
      "\n",
      "Batch 30/134 loss : 15.946553230285645 \n",
      "\n",
      "Batch 31/134 loss : 20.7539119720459 \n",
      "\n",
      "Batch 32/134 loss : 9.253429412841797 \n",
      "\n",
      "Batch 33/134 loss : 8.976753234863281 \n",
      "\n",
      "Batch 34/134 loss : 15.5150785446167 \n",
      "\n",
      "Batch 35/134 loss : 17.145395278930664 \n",
      "\n",
      "Batch 36/134 loss : 19.034393310546875 \n",
      "\n",
      "Batch 37/134 loss : 20.66545867919922 \n",
      "\n",
      "Batch 38/134 loss : 14.566469192504883 \n",
      "\n",
      "Batch 39/134 loss : 14.052590370178223 \n",
      "\n",
      "Batch 40/134 loss : 13.137331008911133 \n",
      "\n",
      "Batch 41/134 loss : 14.68805980682373 \n",
      "\n",
      "Batch 42/134 loss : 19.25118064880371 \n",
      "\n",
      "Batch 43/134 loss : 11.174888610839844 \n",
      "\n",
      "Batch 44/134 loss : 11.728811264038086 \n",
      "\n",
      "Batch 45/134 loss : 12.468974113464355 \n",
      "\n",
      "Batch 46/134 loss : 13.452695846557617 \n",
      "\n",
      "Batch 47/134 loss : 9.88294506072998 \n",
      "\n",
      "Batch 48/134 loss : 17.515995025634766 \n",
      "\n",
      "Batch 49/134 loss : 11.24021053314209 \n",
      "\n",
      "Batch 50/134 loss : 14.792427062988281 \n",
      "\n",
      "Batch 51/134 loss : 17.298437118530273 \n",
      "\n",
      "Batch 52/134 loss : 15.347792625427246 \n",
      "\n",
      "Batch 53/134 loss : 13.017243385314941 \n",
      "\n",
      "Batch 54/134 loss : 9.208968162536621 \n",
      "\n",
      "Batch 55/134 loss : 13.500907897949219 \n",
      "\n",
      "Batch 56/134 loss : 15.78738021850586 \n",
      "\n",
      "Batch 57/134 loss : 13.802887916564941 \n",
      "\n",
      "Batch 58/134 loss : 14.359977722167969 \n",
      "\n",
      "Batch 59/134 loss : 13.479537010192871 \n",
      "\n",
      "Batch 60/134 loss : 14.54042911529541 \n",
      "\n",
      "Batch 61/134 loss : 16.22538185119629 \n",
      "\n",
      "Batch 62/134 loss : 12.046235084533691 \n",
      "\n",
      "Batch 63/134 loss : 20.468774795532227 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 64/134 loss : 12.267443656921387 \n",
      "\n",
      "Batch 65/134 loss : 18.767047882080078 \n",
      "\n",
      "Batch 66/134 loss : 13.422926902770996 \n",
      "\n",
      "Batch 67/134 loss : 9.657476425170898 \n",
      "\n",
      "Batch 68/134 loss : 12.64712142944336 \n",
      "\n",
      "Batch 69/134 loss : 14.786314010620117 \n",
      "\n",
      "Batch 70/134 loss : 10.208247184753418 \n",
      "\n",
      "Batch 71/134 loss : 18.40610122680664 \n",
      "\n",
      "Batch 72/134 loss : 10.694995880126953 \n",
      "\n",
      "Batch 73/134 loss : 14.494327545166016 \n",
      "\n",
      "Batch 74/134 loss : 12.518089294433594 \n",
      "\n",
      "Batch 75/134 loss : 22.81504249572754 \n",
      "\n",
      "Batch 76/134 loss : 15.536825180053711 \n",
      "\n",
      "Batch 77/134 loss : 14.656982421875 \n",
      "\n",
      "Batch 78/134 loss : 16.605697631835938 \n",
      "\n",
      "Batch 79/134 loss : 9.693449020385742 \n",
      "\n",
      "Batch 80/134 loss : 5.513893127441406 \n",
      "\n",
      "Batch 81/134 loss : 14.603771209716797 \n",
      "\n",
      "Batch 82/134 loss : 13.33333683013916 \n",
      "\n",
      "Batch 83/134 loss : 21.19378662109375 \n",
      "\n",
      "Batch 84/134 loss : 8.989269256591797 \n",
      "\n",
      "Batch 85/134 loss : 20.69537353515625 \n",
      "\n",
      "Batch 86/134 loss : 12.951547622680664 \n",
      "\n",
      "Batch 87/134 loss : 15.611388206481934 \n",
      "\n",
      "Batch 88/134 loss : 17.05071258544922 \n",
      "\n",
      "Batch 89/134 loss : 16.643400192260742 \n",
      "\n",
      "Batch 90/134 loss : 11.102649688720703 \n",
      "\n",
      "Batch 91/134 loss : 14.211287498474121 \n",
      "\n",
      "Batch 92/134 loss : 11.723082542419434 \n",
      "\n",
      "Batch 93/134 loss : 12.050578117370605 \n",
      "\n",
      "Batch 94/134 loss : 17.89750862121582 \n",
      "\n",
      "Batch 95/134 loss : 12.959019660949707 \n",
      "\n",
      "Batch 96/134 loss : 11.415539741516113 \n",
      "\n",
      "Batch 97/134 loss : 8.021240234375 \n",
      "\n",
      "Batch 98/134 loss : 12.905892372131348 \n",
      "\n",
      "Batch 99/134 loss : 13.594589233398438 \n",
      "\n",
      "Batch 100/134 loss : 15.094104766845703 \n",
      "\n",
      "Batch 101/134 loss : 12.14173412322998 \n",
      "\n",
      "Batch 102/134 loss : 12.826408386230469 \n",
      "\n",
      "Batch 103/134 loss : 13.591219902038574 \n",
      "\n",
      "Batch 104/134 loss : 12.463889122009277 \n",
      "\n",
      "Batch 105/134 loss : 13.253556251525879 \n",
      "\n",
      "Batch 106/134 loss : 13.308122634887695 \n",
      "\n",
      "Batch 107/134 loss : 6.520027160644531 \n",
      "\n",
      "Batch 108/134 loss : 13.823749542236328 \n",
      "\n",
      "Batch 109/134 loss : 14.22260856628418 \n",
      "\n",
      "Batch 110/134 loss : 17.174991607666016 \n",
      "\n",
      "Batch 111/134 loss : 17.475353240966797 \n",
      "\n",
      "Batch 112/134 loss : 10.541596412658691 \n",
      "\n",
      "Batch 113/134 loss : 10.96109676361084 \n",
      "\n",
      "Batch 114/134 loss : 8.809004783630371 \n",
      "\n",
      "Batch 115/134 loss : 15.400335311889648 \n",
      "\n",
      "Batch 116/134 loss : 20.397747039794922 \n",
      "\n",
      "Batch 117/134 loss : 18.658273696899414 \n",
      "\n",
      "Batch 118/134 loss : 10.10068130493164 \n",
      "\n",
      "Batch 119/134 loss : 19.028215408325195 \n",
      "\n",
      "Batch 120/134 loss : 5.1531758308410645 \n",
      "\n",
      "Batch 121/134 loss : 6.796827793121338 \n",
      "\n",
      "Batch 122/134 loss : 12.95925235748291 \n",
      "\n",
      "Batch 123/134 loss : 10.53438663482666 \n",
      "\n",
      "Batch 124/134 loss : 12.98767375946045 \n",
      "\n",
      "Batch 125/134 loss : 7.269077301025391 \n",
      "\n",
      "Batch 126/134 loss : 10.371084213256836 \n",
      "\n",
      "Batch 127/134 loss : 17.16745948791504 \n",
      "\n",
      "Batch 128/134 loss : 12.844416618347168 \n",
      "\n",
      "Batch 129/134 loss : 13.498188018798828 \n",
      "\n",
      "Batch 130/134 loss : 17.588274002075195 \n",
      "\n",
      "Batch 131/134 loss : 13.13581657409668 \n",
      "\n",
      "Batch 132/134 loss : 13.550281524658203 \n",
      "\n",
      "Batch 133/134 loss : 36.45229721069336 \n",
      "\n",
      "========== Epoch 8 ==========\n",
      "Batch 0/134 loss : 19.615001678466797 \n",
      "\n",
      "Batch 1/134 loss : 10.493782043457031 \n",
      "\n",
      "Batch 2/134 loss : 14.537163734436035 \n",
      "\n",
      "Batch 3/134 loss : 9.183107376098633 \n",
      "\n",
      "Batch 4/134 loss : 14.29074764251709 \n",
      "\n",
      "Batch 5/134 loss : 16.63336753845215 \n",
      "\n",
      "Batch 6/134 loss : 14.09398078918457 \n",
      "\n",
      "Batch 7/134 loss : 13.412713050842285 \n",
      "\n",
      "Batch 8/134 loss : 13.53415298461914 \n",
      "\n",
      "Batch 9/134 loss : 14.72373104095459 \n",
      "\n",
      "Batch 10/134 loss : 9.107202529907227 \n",
      "\n",
      "Batch 11/134 loss : 12.672707557678223 \n",
      "\n",
      "Batch 12/134 loss : 13.956175804138184 \n",
      "\n",
      "Batch 13/134 loss : 10.738832473754883 \n",
      "\n",
      "Batch 14/134 loss : 12.459238052368164 \n",
      "\n",
      "Batch 15/134 loss : 12.23333740234375 \n",
      "\n",
      "Batch 16/134 loss : 4.2450151443481445 \n",
      "\n",
      "Batch 17/134 loss : 15.003288269042969 \n",
      "\n",
      "Batch 18/134 loss : 11.982772827148438 \n",
      "\n",
      "Batch 19/134 loss : 13.643388748168945 \n",
      "\n",
      "Batch 20/134 loss : 10.328900337219238 \n",
      "\n",
      "Batch 21/134 loss : 16.962587356567383 \n",
      "\n",
      "Batch 22/134 loss : 6.161293983459473 \n",
      "\n",
      "Batch 23/134 loss : 18.38237953186035 \n",
      "\n",
      "Batch 24/134 loss : 8.412360191345215 \n",
      "\n",
      "Batch 25/134 loss : 11.644084930419922 \n",
      "\n",
      "Batch 26/134 loss : 15.6579008102417 \n",
      "\n",
      "Batch 27/134 loss : 8.415672302246094 \n",
      "\n",
      "Batch 28/134 loss : 18.25015640258789 \n",
      "\n",
      "Batch 29/134 loss : 10.36881160736084 \n",
      "\n",
      "Batch 30/134 loss : 16.91716957092285 \n",
      "\n",
      "Batch 31/134 loss : 13.786558151245117 \n",
      "\n",
      "Batch 32/134 loss : 10.30548095703125 \n",
      "\n",
      "Batch 33/134 loss : 15.800376892089844 \n",
      "\n",
      "Batch 34/134 loss : 12.767547607421875 \n",
      "\n",
      "Batch 35/134 loss : 13.038174629211426 \n",
      "\n",
      "Batch 36/134 loss : 12.012092590332031 \n",
      "\n",
      "Batch 37/134 loss : 19.32876968383789 \n",
      "\n",
      "Batch 38/134 loss : 17.442495346069336 \n",
      "\n",
      "Batch 39/134 loss : 9.092811584472656 \n",
      "\n",
      "Batch 40/134 loss : 13.77026653289795 \n",
      "\n",
      "Batch 41/134 loss : 6.4900593757629395 \n",
      "\n",
      "Batch 42/134 loss : 12.172167778015137 \n",
      "\n",
      "Batch 43/134 loss : 13.191360473632812 \n",
      "\n",
      "Batch 44/134 loss : 15.559823989868164 \n",
      "\n",
      "Batch 45/134 loss : 9.183049201965332 \n",
      "\n",
      "Batch 46/134 loss : 5.6051836013793945 \n",
      "\n",
      "Batch 47/134 loss : 19.566146850585938 \n",
      "\n",
      "Batch 48/134 loss : 9.810564994812012 \n",
      "\n",
      "Batch 49/134 loss : 12.684818267822266 \n",
      "\n",
      "Batch 50/134 loss : 13.751945495605469 \n",
      "\n",
      "Batch 51/134 loss : 14.15418529510498 \n",
      "\n",
      "Batch 52/134 loss : 15.249653816223145 \n",
      "\n",
      "Batch 53/134 loss : 11.026203155517578 \n",
      "\n",
      "Batch 54/134 loss : 13.246051788330078 \n",
      "\n",
      "Batch 55/134 loss : 11.746057510375977 \n",
      "\n",
      "Batch 56/134 loss : 14.157022476196289 \n",
      "\n",
      "Batch 57/134 loss : 10.754976272583008 \n",
      "\n",
      "Batch 58/134 loss : 14.644412994384766 \n",
      "\n",
      "Batch 59/134 loss : 13.222038269042969 \n",
      "\n",
      "Batch 60/134 loss : 9.967911720275879 \n",
      "\n",
      "Batch 61/134 loss : 15.146860122680664 \n",
      "\n",
      "Batch 62/134 loss : 10.356188774108887 \n",
      "\n",
      "Batch 63/134 loss : 17.466663360595703 \n",
      "\n",
      "Batch 64/134 loss : 17.315372467041016 \n",
      "\n",
      "Batch 65/134 loss : 18.15788459777832 \n",
      "\n",
      "Batch 66/134 loss : 13.745938301086426 \n",
      "\n",
      "Batch 67/134 loss : 14.734877586364746 \n",
      "\n",
      "Batch 68/134 loss : 18.961715698242188 \n",
      "\n",
      "Batch 69/134 loss : 15.354514122009277 \n",
      "\n",
      "Batch 70/134 loss : 15.122295379638672 \n",
      "\n",
      "Batch 71/134 loss : 13.67657470703125 \n",
      "\n",
      "Batch 72/134 loss : 8.267660140991211 \n",
      "\n",
      "Batch 73/134 loss : 10.022396087646484 \n",
      "\n",
      "Batch 74/134 loss : 11.880178451538086 \n",
      "\n",
      "Batch 75/134 loss : 13.834010124206543 \n",
      "\n",
      "Batch 76/134 loss : 17.455034255981445 \n",
      "\n",
      "Batch 77/134 loss : 14.037599563598633 \n",
      "\n",
      "Batch 78/134 loss : 14.827655792236328 \n",
      "\n",
      "Batch 79/134 loss : 15.506176948547363 \n",
      "\n",
      "Batch 80/134 loss : 13.291596412658691 \n",
      "\n",
      "Batch 81/134 loss : 11.571178436279297 \n",
      "\n",
      "Batch 82/134 loss : 9.53659439086914 \n",
      "\n",
      "Batch 83/134 loss : 11.122246742248535 \n",
      "\n",
      "Batch 84/134 loss : 15.875212669372559 \n",
      "\n",
      "Batch 85/134 loss : 10.288406372070312 \n",
      "\n",
      "Batch 86/134 loss : 11.840200424194336 \n",
      "\n",
      "Batch 87/134 loss : 20.98845863342285 \n",
      "\n",
      "Batch 88/134 loss : 19.212158203125 \n",
      "\n",
      "Batch 89/134 loss : 7.932948112487793 \n",
      "\n",
      "Batch 90/134 loss : 16.14750099182129 \n",
      "\n",
      "Batch 91/134 loss : 19.653162002563477 \n",
      "\n",
      "Batch 92/134 loss : 12.967808723449707 \n",
      "\n",
      "Batch 93/134 loss : 10.065807342529297 \n",
      "\n",
      "Batch 94/134 loss : 16.572423934936523 \n",
      "\n",
      "Batch 95/134 loss : 20.071014404296875 \n",
      "\n",
      "Batch 96/134 loss : 11.472685813903809 \n",
      "\n",
      "Batch 97/134 loss : 8.569765090942383 \n",
      "\n",
      "Batch 98/134 loss : 7.533850193023682 \n",
      "\n",
      "Batch 99/134 loss : 15.194173812866211 \n",
      "\n",
      "Batch 100/134 loss : 15.207892417907715 \n",
      "\n",
      "Batch 101/134 loss : 12.783988952636719 \n",
      "\n",
      "Batch 102/134 loss : 6.521476745605469 \n",
      "\n",
      "Batch 103/134 loss : 13.78824234008789 \n",
      "\n",
      "Batch 104/134 loss : 11.371529579162598 \n",
      "\n",
      "Batch 105/134 loss : 9.317150115966797 \n",
      "\n",
      "Batch 106/134 loss : 11.423856735229492 \n",
      "\n",
      "Batch 107/134 loss : 11.073776245117188 \n",
      "\n",
      "Batch 108/134 loss : 10.012596130371094 \n",
      "\n",
      "Batch 109/134 loss : 14.81859016418457 \n",
      "\n",
      "Batch 110/134 loss : 10.748130798339844 \n",
      "\n",
      "Batch 111/134 loss : 17.616806030273438 \n",
      "\n",
      "Batch 112/134 loss : 14.430449485778809 \n",
      "\n",
      "Batch 113/134 loss : 14.924924850463867 \n",
      "\n",
      "Batch 114/134 loss : 11.999086380004883 \n",
      "\n",
      "Batch 115/134 loss : 12.306185722351074 \n",
      "\n",
      "Batch 116/134 loss : 10.56620979309082 \n",
      "\n",
      "Batch 117/134 loss : 15.927633285522461 \n",
      "\n",
      "Batch 118/134 loss : 13.231914520263672 \n",
      "\n",
      "Batch 119/134 loss : 17.63987159729004 \n",
      "\n",
      "Batch 120/134 loss : 8.881162643432617 \n",
      "\n",
      "Batch 121/134 loss : 7.449947357177734 \n",
      "\n",
      "Batch 122/134 loss : 18.59876823425293 \n",
      "\n",
      "Batch 123/134 loss : 9.684552192687988 \n",
      "\n",
      "Batch 124/134 loss : 16.570390701293945 \n",
      "\n",
      "Batch 125/134 loss : 18.18767738342285 \n",
      "\n",
      "Batch 126/134 loss : 12.227067947387695 \n",
      "\n",
      "Batch 127/134 loss : 18.994739532470703 \n",
      "\n",
      "Batch 128/134 loss : 9.77149772644043 \n",
      "\n",
      "Batch 129/134 loss : 12.1097993850708 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 130/134 loss : 9.577512741088867 \n",
      "\n",
      "Batch 131/134 loss : 12.986888885498047 \n",
      "\n",
      "Batch 132/134 loss : 12.687746047973633 \n",
      "\n",
      "Batch 133/134 loss : 8.167486190795898 \n",
      "\n",
      "========== Epoch 9 ==========\n",
      "Batch 0/134 loss : 13.42275619506836 \n",
      "\n",
      "Batch 1/134 loss : 13.505138397216797 \n",
      "\n",
      "Batch 2/134 loss : 9.52198600769043 \n",
      "\n",
      "Batch 3/134 loss : 17.19765853881836 \n",
      "\n",
      "Batch 4/134 loss : 11.031309127807617 \n",
      "\n",
      "Batch 5/134 loss : 10.294811248779297 \n",
      "\n",
      "Batch 6/134 loss : 13.848130226135254 \n",
      "\n",
      "Batch 7/134 loss : 11.39900016784668 \n",
      "\n",
      "Batch 8/134 loss : 13.221242904663086 \n",
      "\n",
      "Batch 9/134 loss : 18.64715576171875 \n",
      "\n",
      "Batch 10/134 loss : 5.751086711883545 \n",
      "\n",
      "Batch 11/134 loss : 14.953400611877441 \n",
      "\n",
      "Batch 12/134 loss : 9.111774444580078 \n",
      "\n",
      "Batch 13/134 loss : 19.002716064453125 \n",
      "\n",
      "Batch 14/134 loss : 13.544921875 \n",
      "\n",
      "Batch 15/134 loss : 7.409868240356445 \n",
      "\n",
      "Batch 16/134 loss : 16.244110107421875 \n",
      "\n",
      "Batch 17/134 loss : 21.64085578918457 \n",
      "\n",
      "Batch 18/134 loss : 8.107471466064453 \n",
      "\n",
      "Batch 19/134 loss : 12.541084289550781 \n",
      "\n",
      "Batch 20/134 loss : 10.614152908325195 \n",
      "\n",
      "Batch 21/134 loss : 10.378406524658203 \n",
      "\n",
      "Batch 22/134 loss : 2.7200026512145996 \n",
      "\n",
      "Batch 23/134 loss : 18.150367736816406 \n",
      "\n",
      "Batch 24/134 loss : 13.01211929321289 \n",
      "\n",
      "Batch 25/134 loss : 11.66900634765625 \n",
      "\n",
      "Batch 26/134 loss : 14.467291831970215 \n",
      "\n",
      "Batch 27/134 loss : 15.536831855773926 \n",
      "\n",
      "Batch 28/134 loss : 20.333206176757812 \n",
      "\n",
      "Batch 29/134 loss : 16.758115768432617 \n",
      "\n",
      "Batch 30/134 loss : 17.30849838256836 \n",
      "\n",
      "Batch 31/134 loss : 12.907633781433105 \n",
      "\n",
      "Batch 32/134 loss : 15.623054504394531 \n",
      "\n",
      "Batch 33/134 loss : 17.13653564453125 \n",
      "\n",
      "Batch 34/134 loss : 16.79168701171875 \n",
      "\n",
      "Batch 35/134 loss : 17.2797908782959 \n",
      "\n",
      "Batch 36/134 loss : 17.009469985961914 \n",
      "\n",
      "Batch 37/134 loss : 15.997912406921387 \n",
      "\n",
      "Batch 38/134 loss : 6.900404930114746 \n",
      "\n",
      "Batch 39/134 loss : 13.422053337097168 \n",
      "\n",
      "Batch 40/134 loss : 18.1315860748291 \n",
      "\n",
      "Batch 41/134 loss : 12.37682819366455 \n",
      "\n",
      "Batch 42/134 loss : 7.952514171600342 \n",
      "\n",
      "Batch 43/134 loss : 9.034326553344727 \n",
      "\n",
      "Batch 44/134 loss : 11.689667701721191 \n",
      "\n",
      "Batch 45/134 loss : 16.69005584716797 \n",
      "\n",
      "Batch 46/134 loss : 13.802072525024414 \n",
      "\n",
      "Batch 47/134 loss : 9.166200637817383 \n",
      "\n",
      "Batch 48/134 loss : 13.690689086914062 \n",
      "\n",
      "Batch 49/134 loss : 8.703317642211914 \n",
      "\n",
      "Batch 50/134 loss : 15.796424865722656 \n",
      "\n",
      "Batch 51/134 loss : 11.517378807067871 \n",
      "\n",
      "Batch 52/134 loss : 16.351274490356445 \n",
      "\n",
      "Batch 53/134 loss : 12.752752304077148 \n",
      "\n",
      "Batch 54/134 loss : 17.936508178710938 \n",
      "\n",
      "Batch 55/134 loss : 12.556131362915039 \n",
      "\n",
      "Batch 56/134 loss : 8.563348770141602 \n",
      "\n",
      "Batch 57/134 loss : 13.8254976272583 \n",
      "\n",
      "Batch 58/134 loss : 10.801606178283691 \n",
      "\n",
      "Batch 59/134 loss : 7.7788004875183105 \n",
      "\n",
      "Batch 60/134 loss : 14.38722038269043 \n",
      "\n",
      "Batch 61/134 loss : 12.039118766784668 \n",
      "\n",
      "Batch 62/134 loss : 11.522672653198242 \n",
      "\n",
      "Batch 63/134 loss : 10.316311836242676 \n",
      "\n",
      "Batch 64/134 loss : 15.753656387329102 \n",
      "\n",
      "Batch 65/134 loss : 12.426067352294922 \n",
      "\n",
      "Batch 66/134 loss : 12.52015495300293 \n",
      "\n",
      "Batch 67/134 loss : 4.471889972686768 \n",
      "\n",
      "Batch 68/134 loss : 12.076292991638184 \n",
      "\n",
      "Batch 69/134 loss : 7.5677080154418945 \n",
      "\n",
      "Batch 70/134 loss : 7.099515914916992 \n",
      "\n",
      "Batch 71/134 loss : 11.215511322021484 \n",
      "\n",
      "Batch 72/134 loss : 7.020318508148193 \n",
      "\n",
      "Batch 73/134 loss : 9.533519744873047 \n",
      "\n",
      "Batch 74/134 loss : 5.761796951293945 \n",
      "\n",
      "Batch 75/134 loss : 6.274262428283691 \n",
      "\n",
      "Batch 76/134 loss : 10.847314834594727 \n",
      "\n",
      "Batch 77/134 loss : 8.311982154846191 \n",
      "\n",
      "Batch 78/134 loss : 12.654389381408691 \n",
      "\n",
      "Batch 79/134 loss : 9.016244888305664 \n",
      "\n",
      "Batch 80/134 loss : 12.875092506408691 \n",
      "\n",
      "Batch 81/134 loss : 14.70242691040039 \n",
      "\n",
      "Batch 82/134 loss : 10.667183876037598 \n",
      "\n",
      "Batch 83/134 loss : 13.606575965881348 \n",
      "\n",
      "Batch 84/134 loss : 10.591415405273438 \n",
      "\n",
      "Batch 85/134 loss : 16.0994873046875 \n",
      "\n",
      "Batch 86/134 loss : 8.568286895751953 \n",
      "\n",
      "Batch 87/134 loss : 9.31589412689209 \n",
      "\n",
      "Batch 88/134 loss : 7.809549808502197 \n",
      "\n",
      "Batch 89/134 loss : 11.534276962280273 \n",
      "\n",
      "Batch 90/134 loss : 16.093326568603516 \n",
      "\n",
      "Batch 91/134 loss : 10.69208812713623 \n",
      "\n",
      "Batch 92/134 loss : 11.117242813110352 \n",
      "\n",
      "Batch 93/134 loss : 15.950387001037598 \n",
      "\n",
      "Batch 94/134 loss : 14.714430809020996 \n",
      "\n",
      "Batch 95/134 loss : 18.499223709106445 \n",
      "\n",
      "Batch 96/134 loss : 15.967913627624512 \n",
      "\n",
      "Batch 97/134 loss : 15.888632774353027 \n",
      "\n",
      "Batch 98/134 loss : 15.183184623718262 \n",
      "\n",
      "Batch 99/134 loss : 10.505715370178223 \n",
      "\n",
      "Batch 100/134 loss : 18.206954956054688 \n",
      "\n",
      "Batch 101/134 loss : 10.018726348876953 \n",
      "\n",
      "Batch 102/134 loss : 14.773880004882812 \n",
      "\n",
      "Batch 103/134 loss : 14.031050682067871 \n",
      "\n",
      "Batch 104/134 loss : 14.734197616577148 \n",
      "\n",
      "Batch 105/134 loss : 8.936088562011719 \n",
      "\n",
      "Batch 106/134 loss : 14.40156364440918 \n",
      "\n",
      "Batch 107/134 loss : 13.338433265686035 \n",
      "\n",
      "Batch 108/134 loss : 14.26009750366211 \n",
      "\n",
      "Batch 109/134 loss : 13.853822708129883 \n",
      "\n",
      "Batch 110/134 loss : 7.460594177246094 \n",
      "\n",
      "Batch 111/134 loss : 17.103273391723633 \n",
      "\n",
      "Batch 112/134 loss : 15.328485488891602 \n",
      "\n",
      "Batch 113/134 loss : 10.141828536987305 \n",
      "\n",
      "Batch 114/134 loss : 14.517474174499512 \n",
      "\n",
      "Batch 115/134 loss : 11.924686431884766 \n",
      "\n",
      "Batch 116/134 loss : 14.652846336364746 \n",
      "\n",
      "Batch 117/134 loss : 15.882268905639648 \n",
      "\n",
      "Batch 118/134 loss : 13.097376823425293 \n",
      "\n",
      "Batch 119/134 loss : 14.929901123046875 \n",
      "\n",
      "Batch 120/134 loss : 11.539695739746094 \n",
      "\n",
      "Batch 121/134 loss : 16.367204666137695 \n",
      "\n",
      "Batch 122/134 loss : 14.796856880187988 \n",
      "\n",
      "Batch 123/134 loss : 11.372819900512695 \n",
      "\n",
      "Batch 124/134 loss : 16.89916229248047 \n",
      "\n",
      "Batch 125/134 loss : 6.79833984375 \n",
      "\n",
      "Batch 126/134 loss : 14.306031227111816 \n",
      "\n",
      "Batch 127/134 loss : 9.951541900634766 \n",
      "\n",
      "Batch 128/134 loss : 8.270217895507812 \n",
      "\n",
      "Batch 129/134 loss : 17.85148811340332 \n",
      "\n",
      "Batch 130/134 loss : 7.7341413497924805 \n",
      "\n",
      "Batch 131/134 loss : 7.086170673370361 \n",
      "\n",
      "Batch 132/134 loss : 8.977295875549316 \n",
      "\n",
      "Batch 133/134 loss : 19.19873809814453 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "model = CBOW(voc_size=batcher.voc_size, embedding_dim=256, window_size=batcher.window_size, batch_size=batcher.batch_size)\n",
    "model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer = optimizer, \\\n",
    "                                 mode = 'min', \\\n",
    "                                 factor = 0.5, \\\n",
    "                                 threshold = 0.001 \\\n",
    "                                )\n",
    "\n",
    "for epoch in range(10):\n",
    "    print('========== Epoch {} =========='.format(epoch))\n",
    "    total_loss = 0\n",
    "    i = 0\n",
    "    N = int(len(batcher.corpus)//BATCH_SIZE)\n",
    "    for context, target in batcher.generator():\n",
    "        model.train()\n",
    "\n",
    "        loss = model(target, context)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Batch {}/{} loss : {}'.format(i, N, loss/(BATCH_SIZE*batcher.window_size*2)), '\\n')\n",
    "        losses.append(loss)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0.5, 'Loss'), Text(0.5, 0, 'Batch')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses2plot = []\n",
    "batches2plot = []\n",
    "for i in range(len(losses)):\n",
    "    losses2plot.append(float(losses[i])/(BATCH_SIZE*batcher.window_size*2))\n",
    "    batches2plot.append(i)\n",
    "sns.lineplot(batches2plot, losses2plot).set(xlabel='Batch', ylabel='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
